{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "classify_AP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "notify_time": "30",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WipL7hIkKB9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a3254e-b9dc-4c13-dfdf-ebec55c70eab"
      },
      "source": [
        "!pip install scikit-multilearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqzJwhWA7n3_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19cbbc72-9e9d-4daf-ddcb-aff8d6d61218"
      },
      "source": [
        "!pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.7/dist-packages (0.10.4)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.2.1)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (0.18.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.1.5)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.18)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.2)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.6)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (5.17.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.13)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (0.2)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.7.3)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.6.0)\n",
            "Requirement already satisfied: strict-rfc3339 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client) (0.7)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: webcolors in /usr/local/lib/python3.7/dist-packages (from jsonschema[format]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client) (1.11.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0SOUKNzBbiY"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score, classification_report, mean_squared_error, confusion_matrix\n",
        "from sklearn.preprocessing import label_binarize, StandardScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from skmultilearn.ensemble import MajorityVotingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from skmultilearn.cluster import FixedLabelSpaceClusterer\n",
        "from skmultilearn.problem_transform import LabelPowerset, BinaryRelevance, ClassifierChain\n",
        "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
        "from skmultilearn.model_selection import iterative_train_test_split, IterativeStratification\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import neptune.new as neptune\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = \"Helvetica\"\n",
        "plt.rcParams['font.family'] = \"sans-serif\"\n",
        "plt.rcParams['font.size'] = 16\n",
        "plt.rcParams['savefig.facecolor'] = 'white'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSwhLLKgh7Ss"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPtmJohWBbic"
      },
      "source": [
        "# Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzSh7weuBbic"
      },
      "source": [
        "def import_data(filepath):\n",
        "    f = h5py.File(filepath, 'r')\n",
        "    V = f['voltage'][...]\n",
        "    Ca = f['calcium'][...]\n",
        "    t = f['time'][...]\n",
        "    labels = f['labels'][...]\n",
        "    return V, Ca, t, labels\n",
        "\n",
        "\n",
        "def normalize_traces(traces_a, traces_b=None):\n",
        "    if traces_b == None:\n",
        "        traces_a_norm = []\n",
        "        for trace in traces_a:\n",
        "            trace_norm = (trace - trace.min()) / (trace.max() - trace.min())\n",
        "            traces_a_norm.append(trace_norm)\n",
        "        traces_a_norm = np.array(traces_a_norm)\n",
        "        return traces_a_norm\n",
        "    else:  # if pass two sets of traces, normalize traces_b relative to traces_a\n",
        "        traces_a_norm, traces_b_norm = [], []\n",
        "        for trace_a, trace_b in zip(traces_a, traces_b):\n",
        "            traces_a_norm.append(\n",
        "                (trace_a - trace_a.min()) / (trace_a.max() - trace_a.min()))\n",
        "            traces_b_norm.append(\n",
        "                (trace_b - trace_a.min()) / (trace_a.max() - trace_a.min()))\n",
        "        traces_a_norm = np.array(traces_a_norm)\n",
        "        traces_b_norm = np.array(traces_b_norm)\n",
        "        return traces_a_norm, traces_b_norm\n",
        "\n",
        "\n",
        "def add_noise(X, percentage=5.0):\n",
        "    std = np.nanmean(X, axis=0).std()\n",
        "    noise = np.random.normal(0, std, X.shape) * percentage / 100\n",
        "    X_noise = X + noise\n",
        "    return X_noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDe9-wqvBbid"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqkkqOUQBbie"
      },
      "source": [
        "#######################################################\n",
        "# To get the data you should download the file from   #\n",
        "# Google Drive and upload it to the working folder    #\n",
        "# here in Google Colab                                #\n",
        "#######################################################\n",
        "\n",
        "data_path = \"/content/data.h5\"\n",
        "\n",
        "\n",
        "label_order = ['Kr', 'CaL', 'Na', 'NaL', 'to', 'Ks', 'K1']\n",
        "\n",
        "V, Ca, t, labels = import_data(data_path)\n",
        "# Separate control and drug cases\n",
        "V_c = V[:, :, 0]\n",
        "V_d = V[:, :, 1]\n",
        "Ca_c = Ca[:, :, 0]\n",
        "Ca_d = Ca[:, :, 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ytulSrmBbie"
      },
      "source": [
        "# Plot data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK7tgp1dBbie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "3502ea02-b2dd-4547-88a2-66c132e498d8"
      },
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
        "fig.suptitle(\"Raw data\")\n",
        "for Vp, Cap in zip(V_c, Ca_c):\n",
        "    ax[0].plot(t, Vp, c='tab:blue', lw=0.4, alpha=0.1)\n",
        "    ax[1].plot(t, Cap, c='darkred', lw=0.4, alpha=0.1)\n",
        "    ax[0].set_ylabel(\"Transmembrane potential\")\n",
        "    ax[1].set_ylabel(\"Calcium\")\n",
        "    ax[1].set_xlabel(\"time (ms)\")\n",
        "    #ax[0].set_xlabel(\"time (ms)\")\n",
        "plt.savefig(\"data.png\", bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_0DUpYWBbif"
      },
      "source": [
        "# Prep data for classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCVyMA0BBbif"
      },
      "source": [
        "# Add noise and combine V with Ca\n",
        "def add_noise_and_normalize(V_c, V_d, Ca_c, Ca_d, pct_noise=0):\n",
        "    #pct_noise = 0.0\n",
        "    V_c_norm, V_d_norm = normalize_traces(add_noise(V_c,pct_noise)), normalize_traces(add_noise(V_d,pct_noise))\n",
        "    Ca_c_norm, Ca_d_norm = normalize_traces(add_noise(Ca_c,pct_noise)), normalize_traces(add_noise(Ca_d,pct_noise))\n",
        "    X = np.concatenate((V_d_norm-V_c_norm,Ca_d_norm-Ca_c_norm),axis=1)\n",
        "\n",
        "    # Standardize data\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    return X\n",
        "\n",
        "noise = 0\n",
        "X = add_noise_and_normalize(V_c, V_d, Ca_c, Ca_d, noise) #, axis=1)\n",
        "y = labels\n",
        "\n",
        "# Split into training and testing\n",
        "#ratio = 0.25 # test/total\n",
        "#indices = np.arange(X_tmp.shape[0])\n",
        "#X, y, X_super_test, y_super_test = iterative_train_test_split(X_tmp, y_tmp, test_size = ratio)\n",
        "# X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y_sparse, indices, test_size=ratio, random_state=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTQXMU7pwIBE"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "def get_labels_for_all_combinations(y):\n",
        "    y_all_combinations = LabelEncoder().fit_transform([''.join(str(l)) for l in y])\n",
        "    return y_all_combinations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhe5JsZVoVE0"
      },
      "source": [
        "# Check list - Classifiers tested:\n",
        "\n",
        "https://app.neptune.ai/o/SSCP/org/SSCP2021/experiments?split=tbl&dash=charts&viewId=standard-view&sortBy=%5B%22Noise%22%5D&sortFieldType=%5B%22int%22%5D&sortFieldAggregationMode=%5B%22auto%22%5D&sortDirection=%5B%22ascending%22%5D&suggestionsEnabled=true&lbViewUnpacked=true\n",
        "\n",
        "### Noise = 0 %\n",
        "- [X] NB-Binary relevance\n",
        "- [X] NB-Label powerset\n",
        "- [X] NB-Classifier chain\n",
        "- [X] SVC-Label powerset\n",
        "- [X] XGBoost - Label powerset\n",
        "- [X] MLP\n",
        "- [X] CNN Encoder\n",
        "- [X] LSTM bidir\n",
        "\n",
        "### Noise = 1 %\n",
        "- [X] NB-Binary relevance\n",
        "- [X] NB-Label powerset\n",
        "- [X] NB-Classifier chain\n",
        "- [X] SVC-Label powerset\n",
        "- [X] XGBoost - Label powerset\n",
        "- [X] MLP\n",
        "- [X] CNN Encoder\n",
        "- [X] LSTM bidir\n",
        "\n",
        "### Noise = 2 %\n",
        "- [X] NB-Binary relevance\n",
        "- [X] NB-Label powerset\n",
        "- [X] NB-Classifier chain\n",
        "- [X] SVC-Label powerset\n",
        "- [X] XGBoost - Label powerset\n",
        "- [X] MLP\n",
        "- [ ] CNN Encoder\n",
        "- [ ] LSTM bidir\n",
        "\n",
        "### Noise = 3 %\n",
        "- [X] NB-Binary relevance\n",
        "- [X] NB-Label powerset\n",
        "- [X] NB-Classifier chain\n",
        "- [X] SVC-Label powerset\n",
        "- [X] XGBoost - Label powerset\n",
        "- [X] MLP\n",
        "- [X] CNN Encoder\n",
        "- [X] LSTM bidir\n",
        "\n",
        "### Noise = 4 %\n",
        "- [X] NB-Binary relevance\n",
        "- [X] NB-Label powerset\n",
        "- [X] NB-Classifier chain\n",
        "- [X] SVC-Label powerset\n",
        "- [ ] XGBoost - Label powerset\n",
        "- [X] MLP\n",
        "- [X] CNN Encoder\n",
        "- [ ] LSTM bidir\n",
        "\n",
        "### Noise = 5 %\n",
        "- [X] NB-Binary relevance\n",
        "- [X] NB-Label powerset\n",
        "- [X] NB-Classifier chain\n",
        "- [X] SVC-Label powerset\n",
        "- [X] XGBoost - Label powerset\n",
        "- [X] MLP\n",
        "- [X] CNN Encoder\n",
        "- [ ] LSTM bidir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp6JCCATBbig"
      },
      "source": [
        "# Train and test classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsvn5OycStko"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "UYJdewvNBbig",
        "outputId": "1180729e-fb2e-40cb-a35d-1dd34f5a54c1"
      },
      "source": [
        "'''\n",
        "clf = BinaryRelevance(classifier = GaussianNB())\n",
        "\n",
        "start = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "print('training time: ',round(time.time()-start,2),'seconds')\n",
        "\n",
        "y_hat = clf.predict(X_test).toarray().astype(int)\n",
        "for i in range(7):\n",
        "    print(f\"Accuracy label {i}:\",accuracy_score(y_test[:,i],y_hat[:,i]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclf = BinaryRelevance(classifier = GaussianNB())\\n\\nstart = time.time()\\nclf.fit(X_train, y_train)\\nprint(\\'training time: \\',round(time.time()-start,2),\\'seconds\\')\\n\\ny_hat = clf.predict(X_test).toarray().astype(int)\\nfor i in range(7):\\n    print(f\"Accuracy label {i}:\",accuracy_score(y_test[:,i],y_hat[:,i]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYZpcB9SUuxx"
      },
      "source": [
        "### BinaryRelevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRFpZWYdhBEJ"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_GaussianNB_BinaryRelevance(X, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"GaussianNB-BinaryRelevance\"\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    run[\"Noise\"] =  noise\n",
        "    \n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = BinaryRelevance(classifier = GaussianNB())\n",
        "        # define search space\n",
        "        parameters = {\n",
        "        'classifier': [GaussianNB()],\n",
        "        'classifier__var_smoothing': np.logspace(0,-9, num=10),\n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = best_model.predict(X_test).toarray()\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: {} sec, noise: {}%'.format(runtime, noise))\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d64O7DXGYR61"
      },
      "source": [
        "run_GaussianNB_BinaryRelevance(X, y, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "9CmUyXiuBbig",
        "outputId": "ddc2939c-f7fd-4a6d-9919-d67ec122f081"
      },
      "source": [
        "'''\n",
        "# Use multi-label classifier chain (MLCC)\n",
        "clf = ClassifierChain( classifier = GaussianNB() )\n",
        "\n",
        "start = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "print('training time: ',round(time.time()-start,2),'seconds')\n",
        "\n",
        "y_hat = clf.predict(X_test).toarray().astype(int)\n",
        "for i in range(7):\n",
        "    print(f\"Accuracy label {i} ({label_order[i]}):\",accuracy_score(y_test[:,i],y_hat[:,i]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Use multi-label classifier chain (MLCC)\\nclf = ClassifierChain( classifier = GaussianNB() )\\n\\nstart = time.time()\\nclf.fit(X_train, y_train)\\nprint(\\'training time: \\',round(time.time()-start,2),\\'seconds\\')\\n\\ny_hat = clf.predict(X_test).toarray().astype(int)\\nfor i in range(7):\\n    print(f\"Accuracy label {i} ({label_order[i]}):\",accuracy_score(y_test[:,i],y_hat[:,i]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE-tj-17VA9b"
      },
      "source": [
        "### ClassifierChain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvg2wIeSiNo-"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_GaussianNB_ClassifierChain(X, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"GaussianNB-ClassifierChain\"\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = ClassifierChain(classifier = GaussianNB())\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'classifier': [GaussianNB()],\n",
        "            'classifier__var_smoothing': np.logspace(0,-9, num=10),\n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = best_model.predict(X_test).toarray()\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    run[\"Noise\"] =  noise\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXSMjVr9ZKFo",
        "outputId": "cb3a7837-e06c-40a1-e3f8-ff522ca458e2"
      },
      "source": [
        "run_GaussianNB_ClassifierChain(X, y, noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-181\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            ">acc=0.013, pres=0.619, rec=0.429, f1=0.437, est=0.026, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.024, pres=0.629, rec=0.395, f1=0.437, est=0.024, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.013, pres=0.643, rec=0.377, f1=0.423, est=0.027, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.020, pres=0.603, rec=0.540, f1=0.520, est=0.023, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.011, pres=0.624, rec=0.503, f1=0.504, est=0.023, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.033, pres=0.638, rec=0.656, f1=0.595, est=0.027, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.024, pres=0.632, rec=0.431, f1=0.454, est=0.023, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.024, pres=0.641, rec=0.405, f1=0.442, est=0.022, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.017, pres=0.616, rec=0.374, f1=0.403, est=0.020, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.1), 'classifier__var_smoothing': 0.1}\n",
            ">acc=0.028, pres=0.638, rec=0.400, f1=0.431, est=0.026, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            "Accuracy: 0.021 (0.055)\n",
            "Runtime: 93.454 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 107 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 107 operations synced, thanks for waiting!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "Ly5aTUfJBbih",
        "outputId": "f9039692-ebda-45ea-8f30-582be831d25a"
      },
      "source": [
        "'''\n",
        "# Formulate as label power-set\n",
        "clf = LabelPowerset( classifier = GaussianNB() )\n",
        "\n",
        "start = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "print('training time: ',round(time.time()-start,2),'seconds')\n",
        "\n",
        "y_hat = clf.predict(X_test).toarray().astype(int)\n",
        "for i in range(7):\n",
        "    print(f\"Accuracy label {i} ({label_order[i]}):\",accuracy_score(y_test[:,i],y_hat[:,i]))\n",
        "    \n",
        "# clf.unique_combinations_\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Formulate as label power-set\\nclf = LabelPowerset( classifier = GaussianNB() )\\n\\nstart = time.time()\\nclf.fit(X_train, y_train)\\nprint(\\'training time: \\',round(time.time()-start,2),\\'seconds\\')\\n\\ny_hat = clf.predict(X_test).toarray().astype(int)\\nfor i in range(7):\\n    print(f\"Accuracy label {i} ({label_order[i]}):\",accuracy_score(y_test[:,i],y_hat[:,i]))\\n    \\n# clf.unique_combinations_\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBWkm2qzVGEa"
      },
      "source": [
        "### LabelPowerset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT4LUfGciZlS"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_GaussianNB_LabelPowerset(X, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"GaussianNB-LabelPowerset\"\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = LabelPowerset(classifier = GaussianNB())\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'classifier': [GaussianNB()],\n",
        "            'classifier__var_smoothing': np.logspace(0,-9, num=10),\n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = best_model.predict(X_test).toarray()\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    run[\"Noise\"] =  noise\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTMUxImoZqqR",
        "outputId": "bef41f13-1771-421b-e876-60705793edcc"
      },
      "source": [
        "run_GaussianNB_LabelPowerset(X, y, noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-182\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            ">acc=0.142, pres=0.745, rec=0.751, f1=0.745, est=0.156, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.170, pres=0.758, rec=0.760, f1=0.753, est=0.158, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1e-06), 'classifier__var_smoothing': 1e-06}\n",
            ">acc=0.133, pres=0.707, rec=0.779, f1=0.739, est=0.168, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1e-07), 'classifier__var_smoothing': 1e-07}\n",
            ">acc=0.146, pres=0.754, rec=0.752, f1=0.752, est=0.141, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.148, pres=0.735, rec=0.753, f1=0.738, est=0.159, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1e-06), 'classifier__var_smoothing': 1e-06}\n",
            ">acc=0.142, pres=0.744, rec=0.765, f1=0.748, est=0.155, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.157, pres=0.740, rec=0.743, f1=0.740, est=0.147, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.133, pres=0.736, rec=0.734, f1=0.733, est=0.151, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.148, pres=0.738, rec=0.774, f1=0.754, est=0.145, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.159, pres=0.727, rec=0.749, f1=0.731, est=0.151, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1e-07), 'classifier__var_smoothing': 1e-07}\n",
            "Accuracy: 0.148 (0.008)\n",
            "Runtime: 188.477 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 109 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 109 operations synced, thanks for waiting!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iXJiEGSBbih"
      },
      "source": [
        "## SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdKXAmTOBbih"
      },
      "source": [
        "\"\"\"\n",
        "# Start with binary relevance\n",
        "clf = BinaryRelevance( classifier = SVC(kernel='linear', C=1, gamma='scale', probability=True) )\n",
        "\n",
        "start = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "print('training time: ',round(time.time()-start,2),'seconds')\n",
        "\n",
        "y_hat = clf.predict(X_test).toarray().astype(int)\n",
        "for i in range(7):\n",
        "    print(f\"Accuracy label {i} ({label_order[i]}):\",accuracy_score(y_test[:,i],y_hat[:,i]))\n",
        "print(\"F1-score:\", f1_score(y_test, yhat, average='macro'))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enwwqDLNlml3"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_SVC_LabelPowerset(X, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"SVC-LabelPowerset\"\n",
        "    run[\"Noise\"] =  noise\n",
        "    \n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = LabelPowerset(classifier = SVC())\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'classifier': [SVC()],\n",
        "            'classifier__C': np.logspace(2,-2, num=3),\n",
        "            'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "            #'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
        "            #https://stackoverflow.com/questions/36306555/scikit-learn-grid-search-with-svm-regression                 \n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "        #print(X_train.shape, kernel_train.shape, y_train.shape)\n",
        "\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = best_model.predict(X_test).toarray()\n",
        "\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "0QEwbFqoaBrP",
        "outputId": "cf4a8dd0-0d40-4db3-ddb7-5c7de0c66cc8"
      },
      "source": [
        "run_SVC_LabelPowerset(X, y, noise)\n",
        "#run_SVC_LabelPowerset(X, y, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-231\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c1509150e6aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_SVC_LabelPowerset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#run_SVC_LabelPowerset(X, y, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b0553a1a2c25>\u001b[0m in \u001b[0;36mrun_SVC_LabelPowerset\u001b[0;34m(X, y, noise)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# execute search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# get the best performing model fit on the whole training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skmultilearn/problem_transform/lp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         self.classifier.fit(self._ensure_input_format(X),\n\u001b[0;32m--> 141\u001b[0;31m                             self.transform(y))\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-USuSIOcJBh0"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB-zzp7YJSVL"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVIEktKuJACx"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_XGBoost_LabelPowerset(X, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"XGBoost-LabelPowerset\"\n",
        "\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = LabelPowerset(classifier = XGBClassifier())\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'classifier': [XGBClassifier()],\n",
        "            'classifier__n_estimators': [1,5],\n",
        "            'classifier__min_child_weight': [1, 5],\n",
        "            'classifier__gamma': [0.5, 1],\n",
        "            'classifier__subsample': [0.8],\n",
        "            'classifier__colsample_bytree': [0.8],\n",
        "            'classifier__max_depth': [3, 5]\n",
        "\n",
        "        }\n",
        "\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = best_model.predict(X_test).toarray()\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    run[\"Noise\"] =  noise\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENr_U1R_aqNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7b3c9f-f195-4331-8f27-1c160189e830"
      },
      "source": [
        "run_XGBoost_LabelPowerset(X, y, noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-229\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFGYxVVOBbii"
      },
      "source": [
        "## MLP neural net with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZvMMWvNa_tu"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao9BUSffOReF"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_MLP_classifier(X, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"MLP-classifier\"\n",
        "\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = MLPClassifier(hidden_layer_sizes=(100,))\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'activation': ['relu'],\n",
        "            'solver': ['adam'],\n",
        "            'alpha': [0.0001],\n",
        "            'batch_size': [50, 100, 200],\n",
        "            'learning_rate_init': [0.001],\n",
        "            'learning_rate': ['adaptive'],\n",
        "            'max_iter': [50,100,200]                 \n",
        "        }\n",
        "\n",
        "        parameter_space = {\n",
        "\n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = best_model.predict(X_test)\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    run[\"Noise\"] =  noise\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhxLY3ZnbRTZ",
        "outputId": "0006f5cf-d7ad-4e3b-993a-7faa3271742f"
      },
      "source": [
        "run_MLP_classifier(X, y, noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-210\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.061, pres=0.697, rec=0.687, f1=0.692, est=0.065, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.061, pres=0.680, rec=0.697, f1=0.689, est=0.063, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.070, pres=0.689, rec=0.700, f1=0.694, est=0.064, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.066, pres=0.695, rec=0.695, f1=0.695, est=0.061, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.066, pres=0.695, rec=0.698, f1=0.696, est=0.066, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.050, pres=0.682, rec=0.692, f1=0.686, est=0.065, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 100, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.074, pres=0.692, rec=0.684, f1=0.688, est=0.066, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.057, pres=0.676, rec=0.668, f1=0.672, est=0.068, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.044, pres=0.697, rec=0.704, f1=0.700, est=0.063, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">acc=0.052, pres=0.684, rec=0.682, f1=0.682, est=0.062, cfg={'activation': 'relu', 'alpha': 0.0001, 'batch_size': 200, 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 50, 'solver': 'adam'}\n",
            "Accuracy: 0.060 (0.008)\n",
            "Runtime: 2287.517 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "Waiting for the remaining 108 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 108 operations synced, thanks for waiting!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "5vs-92jwBbii",
        "outputId": "7055dc85-a61a-4fea-9b7f-938641f85d91"
      },
      "source": [
        "'''\n",
        "def get_model(n_inputs, n_outputs):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def evaluate_model(X, y):\n",
        "    results = list()\n",
        "    results_class = list()\n",
        "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "    # define evaluation procedure\n",
        "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    # enumerate folds\n",
        "    for train_ix, test_ix in tqdm(cv.split(X), total=30):\n",
        "        # prepare data\n",
        "        X_train, X_test = X[train_ix], X[test_ix]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # define model\n",
        "        model = get_model(n_inputs, n_outputs)\n",
        "        # fit model\n",
        "        model.fit(X_train, y_train, verbose=0, epochs=100)\n",
        "        # make a prediction on the test set\n",
        "        y_hat = model.predict(X_test)\n",
        "        # round probabilities to class labels\n",
        "        y_hat = y_hat.round()\n",
        "        # calculate accuracy\n",
        "        acc = accuracy_score(y_test, y_hat)\n",
        "        class_acc = np.zeros(7)\n",
        "        for i in range(7):\n",
        "          class_acc[i] = accuracy_score(y_test[:,i],y_hat[:,i])\n",
        "        # store result\n",
        "        results.append(acc)\n",
        "        results_class.append(class_acc)\n",
        "    return results, results_class\n",
        "\n",
        "results, results_class = evaluate_model(X, y)\n",
        "# summarize performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(results), np.std(results)))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef get_model(n_inputs, n_outputs):\\n    model = Sequential()\\n    model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\\n    model.add(Dense(n_outputs, activation='sigmoid'))\\n    model.compile(loss='binary_crossentropy', optimizer='adam')\\n    return model\\n\\ndef evaluate_model(X, y):\\n    results = list()\\n    results_class = list()\\n    n_inputs, n_outputs = X.shape[1], y.shape[1]\\n    # define evaluation procedure\\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\\n    # enumerate folds\\n    for train_ix, test_ix in tqdm(cv.split(X), total=30):\\n        # prepare data\\n        X_train, X_test = X[train_ix], X[test_ix]\\n        y_train, y_test = y[train_ix], y[test_ix]\\n        # define model\\n        model = get_model(n_inputs, n_outputs)\\n        # fit model\\n        model.fit(X_train, y_train, verbose=0, epochs=100)\\n        # make a prediction on the test set\\n        y_hat = model.predict(X_test)\\n        # round probabilities to class labels\\n        y_hat = y_hat.round()\\n        # calculate accuracy\\n        acc = accuracy_score(y_test, y_hat)\\n        class_acc = np.zeros(7)\\n        for i in range(7):\\n          class_acc[i] = accuracy_score(y_test[:,i],y_hat[:,i])\\n        # store result\\n        results.append(acc)\\n        results_class.append(class_acc)\\n    return results, results_class\\n\\nresults, results_class = evaluate_model(X, y)\\n# summarize performance\\nprint('Accuracy: %.3f (%.3f)' % (np.mean(results), np.std(results)))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ui0cF2MnZ6r-",
        "outputId": "b17ee752-7e0c-4679-9198-92211abd6f33"
      },
      "source": [
        "'''\n",
        "results_class = np.asarray(results_class)\n",
        "for i in range(7):\n",
        "  print(f\"Accuracy label {i} - Mean: { results_class[:,i].mean() }, SD:{results_class[:,i].std()} \")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nresults_class = np.asarray(results_class)\\nfor i in range(7):\\n  print(f\"Accuracy label {i} - Mean: { results_class[:,i].mean() }, SD:{results_class[:,i].std()} \")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YF3EzOTbtZE"
      },
      "source": [
        "## Run all the stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WVpSlu1CbxBF",
        "outputId": "37ab4e62-ec91-4993-ea4b-edf2cbf2e110"
      },
      "source": [
        "#X0 = add_noise_and_normalize(V_c, V_d, Ca_c, Ca_d, pct_noise=2)\n",
        "#X1 = add_noise_and_normalize(V_c, V_d, Ca_c, Ca_d, pct_noise=3.5)\n",
        "#X2 = add_noise_and_normalize(V_c, V_d, Ca_c, Ca_d, pct_noise=5)\n",
        "\n",
        "\"\"\"\n",
        "This cell block runs all the methods above with diifernt levels of noise.\n",
        "I'm quite sure how best to add the CNN and RNN models here.\n",
        "If you create another method that fits here, feel free to add it.\n",
        "\"\"\"\n",
        "\n",
        "#noises = [2, 3, 4, 5]\n",
        "noises = [3, 4, 5]\n",
        "for i in range(len(noises)):\n",
        "    X_curr = add_noise_and_normalize(V_c, V_d, Ca_c, Ca_d, pct_noise=noises[i])\n",
        "\n",
        "    print(\"\\nGaussianNB BinaryRelevance, noise={}%.\".format(noises[i]))\n",
        "    run_GaussianNB_BinaryRelevance(X_curr, y, noises[i])\n",
        "\n",
        "    print(\"\\nGaussianNB ClassifierChain, noise={}%.\".format(noises[i]))\n",
        "    run_GaussianNB_ClassifierChain(X_curr, y, noises[i])\n",
        "\n",
        "    print(\"\\nGaussianNB LabelPowerset, noise={}%.\".format(noises[i]))\n",
        "    run_GaussianNB_LabelPowerset(X_curr, y, noises[i])\n",
        "\n",
        "    print(\"\\nSVC, noise={}%.\".format(noises[i]))\n",
        "    run_SVC_LabelPowerset(X_curr, y, noises[i])\n",
        "    \n",
        "    print(\"\\nXGBoost, noise={}%.\".format(noises[i]))\n",
        "    run_XGBoost_LabelPowerset(X_curr, y, noises[i])\n",
        "\n",
        "    print(\"\\nMLP, noise={}%.\".format(noises[i]))\n",
        "    run_MLP_classifier(X_curr, y, noises[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "GaussianNB BinaryRelevance, noise=3%.\n",
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-203\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            ">acc=0.017, pres=0.603, rec=0.394, f1=0.441, est=0.023, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.013, pres=0.588, rec=0.435, f1=0.475, est=0.015, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.022, pres=0.605, rec=0.401, f1=0.456, est=0.022, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.024, pres=0.611, rec=0.543, f1=0.536, est=0.021, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.1), 'classifier__var_smoothing': 0.1}\n",
            ">acc=0.017, pres=0.618, rec=0.475, f1=0.485, est=0.019, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.013, pres=0.622, rec=0.559, f1=0.563, est=0.024, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.015, pres=0.601, rec=0.450, f1=0.479, est=0.018, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.024, pres=0.641, rec=0.497, f1=0.535, est=0.019, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.017, pres=0.617, rec=0.464, f1=0.513, est=0.021, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.026, pres=0.630, rec=0.399, f1=0.453, est=0.024, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            "Accuracy: 0.019 (0.039)\n",
            "Runtime: 55.73673629760742 sec, noise: 3%\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 109 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 109 operations synced, thanks for waiting!\n",
            "\n",
            "GaussianNB ClassifierChain, noise=3%.\n",
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-204\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            ">acc=0.017, pres=0.603, rec=0.394, f1=0.441, est=0.023, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.013, pres=0.588, rec=0.435, f1=0.475, est=0.015, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.022, pres=0.605, rec=0.401, f1=0.456, est=0.022, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.024, pres=0.612, rec=0.543, f1=0.536, est=0.021, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.1), 'classifier__var_smoothing': 0.1}\n",
            ">acc=0.017, pres=0.618, rec=0.475, f1=0.485, est=0.019, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.013, pres=0.622, rec=0.559, f1=0.563, est=0.024, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.015, pres=0.602, rec=0.451, f1=0.480, est=0.018, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            ">acc=0.024, pres=0.641, rec=0.499, f1=0.535, est=0.019, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.017, pres=0.618, rec=0.465, f1=0.514, est=0.021, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.026, pres=0.632, rec=0.400, f1=0.455, est=0.024, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n",
            "Accuracy: 0.019 (0.039)\n",
            "Runtime: 79.697 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 107 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 107 operations synced, thanks for waiting!\n",
            "\n",
            "GaussianNB LabelPowerset, noise=3%.\n",
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-205\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            ">acc=0.037, pres=0.645, rec=0.570, f1=0.603, est=0.046, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.068, pres=0.639, rec=0.597, f1=0.616, est=0.050, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.044, pres=0.630, rec=0.622, f1=0.625, est=0.052, cfg={'classifier': GaussianNB(priors=None, var_smoothing=1e-05), 'classifier__var_smoothing': 1e-05}\n",
            ">acc=0.046, pres=0.646, rec=0.585, f1=0.613, est=0.045, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.055, pres=0.640, rec=0.597, f1=0.616, est=0.049, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.037, pres=0.634, rec=0.595, f1=0.612, est=0.052, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            ">acc=0.061, pres=0.632, rec=0.593, f1=0.608, est=0.048, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.052, pres=0.632, rec=0.598, f1=0.611, est=0.048, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.01), 'classifier__var_smoothing': 0.01}\n",
            ">acc=0.063, pres=0.646, rec=0.595, f1=0.617, est=0.049, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.0001), 'classifier__var_smoothing': 0.0001}\n",
            ">acc=0.050, pres=0.646, rec=0.616, f1=0.628, est=0.048, cfg={'classifier': GaussianNB(priors=None, var_smoothing=0.001), 'classifier__var_smoothing': 0.001}\n",
            "Accuracy: 0.051 (0.007)\n",
            "Runtime: 137.386 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 107 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 107 operations synced, thanks for waiting!\n",
            "\n",
            "XGBoost, noise=3%.\n",
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-206\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a923d7e12262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nXGBoost, noise={}%.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrun_XGBoost_LabelPowerset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMLP, noise={}%.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoises\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-57b0fa9a686a>\u001b[0m in \u001b[0;36mrun_XGBoost_LabelPowerset\u001b[0;34m(X, y, noise)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# execute search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# get the best performing model fit on the whole training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skmultilearn/problem_transform/lp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         self.classifier.fit(self._ensure_input_format(X),\n\u001b[0;32m--> 141\u001b[0;31m                             self.transform(y))\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50J0FNRVHDYK"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkGmxTQJPdi5"
      },
      "source": [
        "def encoder_model(n_input, n_output):\n",
        "    X_input=tf.keras.layers.Input(shape=(n_input))\n",
        "     # conv block -1\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(X_input)\n",
        "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "    conv1 = tf.keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "    conv1 = tf.keras.layers.Dropout(rate=0.2)(conv1)\n",
        "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "    # conv block -2\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
        "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "    conv2 = tf.keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "    conv2 = tf.keras.layers.Dropout(rate=0.2)(conv2)\n",
        "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "    # conv block -3\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
        "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "    conv3 = tf.keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "    conv3 = tf.keras.layers.Dropout(rate=0.2)(conv3)\n",
        "    # split for attention\n",
        "    attention_data = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n",
        "    attention_softmax = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n",
        "    # attention mechanism\n",
        "    attention_softmax = tf.keras.layers.Softmax()(attention_softmax)\n",
        "    multiply_layer = tf.keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "    # last layer\n",
        "    dense_layer = tf.keras.layers.Dense(units=512,activation='sigmoid')(multiply_layer)\n",
        "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "    # output layer\n",
        "    flatten_layer = tf.keras.layers.Flatten()(dense_layer)\n",
        "    output_layer = tf.keras.layers.Dense(units=n_output,activation='sigmoid')(flatten_layer)\n",
        "\n",
        "    model = tf.keras.Model(inputs=X_input, outputs=output_layer)\n",
        "\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
        "                  #,tfa.metrics.F1Score(num_classes=n_output)\n",
        "                  ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpEAqUYhBJQR"
      },
      "source": [
        "def encoder_model_2():\n",
        "    X_input=tf.keras.layers.Input(shape=(200,2))\n",
        "     # conv block -1\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(X_input)\n",
        "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "    conv1 = tf.keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "    conv1 = tf.keras.layers.Dropout(rate=0.2)(conv1)\n",
        "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "    # conv block -2\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
        "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "    conv2 = tf.keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "    conv2 = tf.keras.layers.Dropout(rate=0.2)(conv2)\n",
        "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "    # conv block -3\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
        "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "    conv3 = tf.keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "    conv3 = tf.keras.layers.Dropout(rate=0.2)(conv3)\n",
        "    # split for attention\n",
        "    attention_data = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n",
        "    attention_softmax = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n",
        "    # attention mechanism\n",
        "    attention_softmax = tf.keras.layers.Softmax()(attention_softmax)\n",
        "    multiply_layer = tf.keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "    # last layer\n",
        "    dense_layer = tf.keras.layers.Dense(units=512,activation='sigmoid')(multiply_layer)\n",
        "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "    # output layer\n",
        "    flatten_layer = tf.keras.layers.Flatten()(dense_layer)\n",
        "    output_layer = tf.keras.layers.Dense(units=7,activation='sigmoid')(flatten_layer)\n",
        "\n",
        "    model = tf.keras.Model(inputs=X_input, outputs=output_layer)\n",
        "\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQHhpTbCPENM",
        "outputId": "9f50d28f-46da-4d73-8f27-6a3341d74293"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77RQsalYHRDo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HktNUNmKJKAY"
      },
      "source": [
        "# Add noise and combine V with Ca\n",
        "def add_noise_and_normalize_new_axis(V_c, V_d, Ca_c, Ca_d, pct_noise=0):\n",
        "    #pct_noise = 0.0\n",
        "    V_c_norm, V_d_norm = normalize_traces(add_noise(V_c,pct_noise)), normalize_traces(add_noise(V_d,pct_noise))\n",
        "    Ca_c_norm, Ca_d_norm = normalize_traces(add_noise(Ca_c,pct_noise)), normalize_traces(add_noise(Ca_d,pct_noise))\n",
        "    V_diff = V_d_norm-V_c_norm\n",
        "    Ca_diff = Ca_d_norm-Ca_c_norm\n",
        "    scaler_V = StandardScaler()\n",
        "    scaler_Ca = StandardScaler()\n",
        "    X_v = scaler_V.fit_transform(V_diff)\n",
        "    X_Ca = scaler_Ca.fit_transform(Ca_diff)\n",
        "\n",
        "\n",
        "    X = np.concatenate((np.expand_dims(X_v,axis = 2),np.expand_dims(X_Ca,axis = 2)) ,axis=2)\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYq56kq1H_jj"
      },
      "source": [
        "X_new = add_noise_and_normalize_new_axis(V_c, V_d, Ca_c, Ca_d, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX-HpQQBLNIZ"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_CNN(X_new, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"CNN-model - Encoder\"\n",
        "    run[\"Data\"] = \"Ca and V on separate axis\"\n",
        "    run[\"Noise\"] =  noise\n",
        "\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X_new,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X_new[train_ix, :, :], X_new[test_ix, :,:]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = KerasClassifier(build_fn=encoder_model_2, verbose=1)\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'epochs': [20,60,100],\n",
        "            'batch_size': [20,60,100]\n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, cv=cv_inner, refit=True)\n",
        "        #search = GridSearchCV(estimator=model, param_grid=parameters)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "\n",
        "        yhat = (best_model.predict_proba(X_test) > 0.5) * 1\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHs96ImRODA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b2517f-ea44-4b47-92d1-722e66b5b32c"
      },
      "source": [
        "run_CNN(X_new, y, noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0184 - accuracy: 0.9934\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0185 - accuracy: 0.9943\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0196 - accuracy: 0.9935\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0207 - accuracy: 0.9923\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0203 - accuracy: 0.9933\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0209 - accuracy: 0.9932\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0192 - accuracy: 0.9932\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0172 - accuracy: 0.9943\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0190 - accuracy: 0.9933\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0192 - accuracy: 0.9942\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0178 - accuracy: 0.9939\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0195 - accuracy: 0.9931\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0197 - accuracy: 0.9940\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0155 - accuracy: 0.9943\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0174 - accuracy: 0.9939\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0185 - accuracy: 0.9939\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0202 - accuracy: 0.9931\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0175 - accuracy: 0.9945\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0156 - accuracy: 0.9950\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9946\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0135 - accuracy: 0.9949\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0131 - accuracy: 0.9957\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0107 - accuracy: 0.9963\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0120 - accuracy: 0.9953\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0141 - accuracy: 0.9952\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0093 - accuracy: 0.9973\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0120 - accuracy: 0.9958\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0115 - accuracy: 0.9962\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0125 - accuracy: 0.9960\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9948\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0143 - accuracy: 0.9952\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0124 - accuracy: 0.9958\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9948\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0139 - accuracy: 0.9953\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0130 - accuracy: 0.9960\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0124 - accuracy: 0.9964\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0112 - accuracy: 0.9959\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0123 - accuracy: 0.9956\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0145 - accuracy: 0.9948\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0175 - accuracy: 0.9946\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0162 - accuracy: 0.9949\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0203 - accuracy: 0.9935\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0146 - accuracy: 0.9952\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0146 - accuracy: 0.9958\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9954\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0151 - accuracy: 0.9956\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0158 - accuracy: 0.9942\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0196 - accuracy: 0.9939\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0156 - accuracy: 0.9948\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0128 - accuracy: 0.9961\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0088 - accuracy: 0.9969\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0098 - accuracy: 0.9970\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0093 - accuracy: 0.9972\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0087 - accuracy: 0.9968\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0106 - accuracy: 0.9964\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0088 - accuracy: 0.9969\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.8783 - accuracy: 0.7934\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.5026 - accuracy: 0.7091\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3868 - accuracy: 0.7891\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3481 - accuracy: 0.8191\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3109 - accuracy: 0.8437\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2784 - accuracy: 0.8654\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2370 - accuracy: 0.8873\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2000 - accuracy: 0.9092\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1786 - accuracy: 0.9198\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1466 - accuracy: 0.9375\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1122 - accuracy: 0.9551\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0939 - accuracy: 0.9622\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0779 - accuracy: 0.9692\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0661 - accuracy: 0.9744\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0532 - accuracy: 0.9804\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0482 - accuracy: 0.9818\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0414 - accuracy: 0.9845\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0323 - accuracy: 0.9884\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0403 - accuracy: 0.9861\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0373 - accuracy: 0.9867\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0400 - accuracy: 0.9855\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0364 - accuracy: 0.9866\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0320 - accuracy: 0.9884\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0282 - accuracy: 0.9896\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0232 - accuracy: 0.9918\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0252 - accuracy: 0.9914\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0265 - accuracy: 0.9898\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0244 - accuracy: 0.9916\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0274 - accuracy: 0.9901\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0233 - accuracy: 0.9909\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0211 - accuracy: 0.9926\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0223 - accuracy: 0.9916\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0236 - accuracy: 0.9915\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0251 - accuracy: 0.9914\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0252 - accuracy: 0.9903\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0230 - accuracy: 0.9921\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0218 - accuracy: 0.9927\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0242 - accuracy: 0.9916\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0214 - accuracy: 0.9927\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0240 - accuracy: 0.9913\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0222 - accuracy: 0.9926\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0233 - accuracy: 0.9918\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0221 - accuracy: 0.9929\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0177 - accuracy: 0.9936\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0217 - accuracy: 0.9929\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0159 - accuracy: 0.9940\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0147 - accuracy: 0.9948\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9952\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0168 - accuracy: 0.9942\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0163 - accuracy: 0.9950\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0169 - accuracy: 0.9946\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9947\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9943\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0138 - accuracy: 0.9961\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0145 - accuracy: 0.9950\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0221 - accuracy: 0.9936\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0167 - accuracy: 0.9942\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0164 - accuracy: 0.9944\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0154 - accuracy: 0.9952\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0129 - accuracy: 0.9956\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0147 - accuracy: 0.9951\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0161 - accuracy: 0.9944\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0163 - accuracy: 0.9945\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0166 - accuracy: 0.9943\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0171 - accuracy: 0.9940\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0146 - accuracy: 0.9950\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0164 - accuracy: 0.9950\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0144 - accuracy: 0.9944\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0167 - accuracy: 0.9940\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0140 - accuracy: 0.9957\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0140 - accuracy: 0.9961\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0146 - accuracy: 0.9947\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0179 - accuracy: 0.9944\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0151 - accuracy: 0.9945\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9946\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9948\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0139 - accuracy: 0.9947\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0115 - accuracy: 0.9961\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0144 - accuracy: 0.9951\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0137 - accuracy: 0.9956\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0120 - accuracy: 0.9957\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0125 - accuracy: 0.9957\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0123 - accuracy: 0.9958\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0141 - accuracy: 0.9956\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0096 - accuracy: 0.9964\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0103 - accuracy: 0.9965\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9964\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0080 - accuracy: 0.9971\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0099 - accuracy: 0.9967\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0135 - accuracy: 0.9957\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0160 - accuracy: 0.9954\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0100 - accuracy: 0.9965\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0109 - accuracy: 0.9963\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0119 - accuracy: 0.9962\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0132 - accuracy: 0.9963\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0119 - accuracy: 0.9957\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0126 - accuracy: 0.9959\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0090 - accuracy: 0.9963\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.9024 - accuracy: 0.7876\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 3s 27ms/step - loss: 0.5155 - accuracy: 0.7014\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3899 - accuracy: 0.7903\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3488 - accuracy: 0.8187\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3036 - accuracy: 0.8496\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2707 - accuracy: 0.8679\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2309 - accuracy: 0.8908\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2029 - accuracy: 0.9079\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1627 - accuracy: 0.9304\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1367 - accuracy: 0.9421\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1139 - accuracy: 0.9529\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0981 - accuracy: 0.9617\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0843 - accuracy: 0.9667\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0702 - accuracy: 0.9745\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0568 - accuracy: 0.9798\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0510 - accuracy: 0.9809\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0421 - accuracy: 0.9856\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0344 - accuracy: 0.9881\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0353 - accuracy: 0.9872\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0329 - accuracy: 0.9878\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0360 - accuracy: 0.9877\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4962 - accuracy: 0.7989\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5155 - accuracy: 0.6998\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3911 - accuracy: 0.7919\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.3425 - accuracy: 0.8237\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3116 - accuracy: 0.8408\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2726 - accuracy: 0.8677\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2372 - accuracy: 0.8869\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2027 - accuracy: 0.9079\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1638 - accuracy: 0.9288\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1414 - accuracy: 0.9418\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1114 - accuracy: 0.9552\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0991 - accuracy: 0.9621\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0782 - accuracy: 0.9693\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0626 - accuracy: 0.9765\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0535 - accuracy: 0.9803\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0479 - accuracy: 0.9832\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0400 - accuracy: 0.9855\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0379 - accuracy: 0.9873\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0378 - accuracy: 0.9854\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0311 - accuracy: 0.9893\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0288 - accuracy: 0.9891\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.5324 - accuracy: 0.7867\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 3s 27ms/step - loss: 0.5164 - accuracy: 0.7013\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3938 - accuracy: 0.7907\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3439 - accuracy: 0.8239\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3008 - accuracy: 0.8486\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2650 - accuracy: 0.8723\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2329 - accuracy: 0.8927\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1932 - accuracy: 0.9144\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.1571 - accuracy: 0.9344\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1346 - accuracy: 0.9442\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1127 - accuracy: 0.9539\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0828 - accuracy: 0.9695\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0638 - accuracy: 0.9775\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0541 - accuracy: 0.9810\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0503 - accuracy: 0.9820\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0400 - accuracy: 0.9863\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0347 - accuracy: 0.9881\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0340 - accuracy: 0.9873\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0337 - accuracy: 0.9884\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0359 - accuracy: 0.9881\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0354 - accuracy: 0.9874\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.5283 - accuracy: 0.7869\n",
            "Epoch 1/60\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5149 - accuracy: 0.7006\n",
            "Epoch 2/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3925 - accuracy: 0.7866\n",
            "Epoch 3/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.3511 - accuracy: 0.8145\n",
            "Epoch 4/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.3055 - accuracy: 0.8469\n",
            "Epoch 5/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2752 - accuracy: 0.8652\n",
            "Epoch 6/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2349 - accuracy: 0.8904\n",
            "Epoch 7/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2038 - accuracy: 0.9087\n",
            "Epoch 8/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1711 - accuracy: 0.9249\n",
            "Epoch 9/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1434 - accuracy: 0.9420\n",
            "Epoch 10/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1156 - accuracy: 0.9517\n",
            "Epoch 11/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0915 - accuracy: 0.9636\n",
            "Epoch 12/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0787 - accuracy: 0.9698\n",
            "Epoch 13/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0659 - accuracy: 0.9746\n",
            "Epoch 14/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0561 - accuracy: 0.9788\n",
            "Epoch 15/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0515 - accuracy: 0.9821\n",
            "Epoch 16/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0409 - accuracy: 0.9857\n",
            "Epoch 17/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0373 - accuracy: 0.9860\n",
            "Epoch 18/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0355 - accuracy: 0.9879\n",
            "Epoch 19/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0309 - accuracy: 0.9893\n",
            "Epoch 20/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0313 - accuracy: 0.9895\n",
            "Epoch 21/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0287 - accuracy: 0.9898\n",
            "Epoch 22/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0323 - accuracy: 0.9887\n",
            "Epoch 23/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0296 - accuracy: 0.9895\n",
            "Epoch 24/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0270 - accuracy: 0.9904\n",
            "Epoch 25/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0257 - accuracy: 0.9919\n",
            "Epoch 26/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0235 - accuracy: 0.9919\n",
            "Epoch 27/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0185 - accuracy: 0.9940\n",
            "Epoch 28/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0178 - accuracy: 0.9933\n",
            "Epoch 29/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0189 - accuracy: 0.9933\n",
            "Epoch 30/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0209 - accuracy: 0.9926\n",
            "Epoch 31/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0218 - accuracy: 0.9924\n",
            "Epoch 32/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0187 - accuracy: 0.9930\n",
            "Epoch 33/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0212 - accuracy: 0.9926\n",
            "Epoch 34/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0202 - accuracy: 0.9930\n",
            "Epoch 35/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0219 - accuracy: 0.9922\n",
            "Epoch 36/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0191 - accuracy: 0.9939\n",
            "Epoch 37/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0190 - accuracy: 0.9940\n",
            "Epoch 38/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0192 - accuracy: 0.9933\n",
            "Epoch 39/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0197 - accuracy: 0.9930\n",
            "Epoch 40/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9949\n",
            "Epoch 41/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0169 - accuracy: 0.9939\n",
            "Epoch 42/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0189 - accuracy: 0.9940\n",
            "Epoch 43/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0189 - accuracy: 0.9943\n",
            "Epoch 44/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0171 - accuracy: 0.9939\n",
            "Epoch 45/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0166 - accuracy: 0.9940\n",
            "Epoch 46/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9939\n",
            "Epoch 47/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9951\n",
            "Epoch 48/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0155 - accuracy: 0.9940\n",
            "Epoch 49/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0151 - accuracy: 0.9945\n",
            "Epoch 50/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9950\n",
            "Epoch 51/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0148 - accuracy: 0.9945\n",
            "Epoch 52/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0111 - accuracy: 0.9962\n",
            "Epoch 53/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9951\n",
            "Epoch 54/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0132 - accuracy: 0.9955\n",
            "Epoch 55/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 56/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9954\n",
            "Epoch 57/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9953\n",
            "Epoch 58/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0141 - accuracy: 0.9952\n",
            "Epoch 59/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0124 - accuracy: 0.9960\n",
            "Epoch 60/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0129 - accuracy: 0.9954\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.7945\n",
            "Epoch 1/60\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5163 - accuracy: 0.7012\n",
            "Epoch 2/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3903 - accuracy: 0.7909\n",
            "Epoch 3/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3459 - accuracy: 0.8221\n",
            "Epoch 4/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3046 - accuracy: 0.8448\n",
            "Epoch 5/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2687 - accuracy: 0.8701\n",
            "Epoch 6/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2347 - accuracy: 0.8896\n",
            "Epoch 7/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1974 - accuracy: 0.9132\n",
            "Epoch 8/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1631 - accuracy: 0.9305\n",
            "Epoch 9/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1351 - accuracy: 0.9434\n",
            "Epoch 10/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.1112 - accuracy: 0.9569\n",
            "Epoch 11/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0949 - accuracy: 0.9642\n",
            "Epoch 12/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0769 - accuracy: 0.9709\n",
            "Epoch 13/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0649 - accuracy: 0.9758\n",
            "Epoch 14/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0554 - accuracy: 0.9798\n",
            "Epoch 15/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0479 - accuracy: 0.9826\n",
            "Epoch 16/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0394 - accuracy: 0.9859\n",
            "Epoch 17/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0350 - accuracy: 0.9872\n",
            "Epoch 18/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0277 - accuracy: 0.9901\n",
            "Epoch 19/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0310 - accuracy: 0.9893\n",
            "Epoch 20/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0310 - accuracy: 0.9888\n",
            "Epoch 21/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0311 - accuracy: 0.9894\n",
            "Epoch 22/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0251 - accuracy: 0.9915\n",
            "Epoch 23/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0256 - accuracy: 0.9908\n",
            "Epoch 24/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0268 - accuracy: 0.9902\n",
            "Epoch 25/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0248 - accuracy: 0.9910\n",
            "Epoch 26/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0226 - accuracy: 0.9920\n",
            "Epoch 27/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0253 - accuracy: 0.9914\n",
            "Epoch 28/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0253 - accuracy: 0.9910\n",
            "Epoch 29/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0245 - accuracy: 0.9913\n",
            "Epoch 30/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0256 - accuracy: 0.9916\n",
            "Epoch 31/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0215 - accuracy: 0.9928\n",
            "Epoch 32/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0221 - accuracy: 0.9925\n",
            "Epoch 33/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0249 - accuracy: 0.9917\n",
            "Epoch 34/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0203 - accuracy: 0.9926\n",
            "Epoch 35/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0193 - accuracy: 0.9931\n",
            "Epoch 36/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0186 - accuracy: 0.9930\n",
            "Epoch 37/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0182 - accuracy: 0.9932\n",
            "Epoch 38/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0137 - accuracy: 0.9950\n",
            "Epoch 39/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0136 - accuracy: 0.9955\n",
            "Epoch 40/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9957\n",
            "Epoch 41/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0132 - accuracy: 0.9959\n",
            "Epoch 42/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9952\n",
            "Epoch 43/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0157 - accuracy: 0.9945\n",
            "Epoch 44/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9939\n",
            "Epoch 45/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0187 - accuracy: 0.9937\n",
            "Epoch 46/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0144 - accuracy: 0.9947\n",
            "Epoch 47/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0155 - accuracy: 0.9944\n",
            "Epoch 48/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9943\n",
            "Epoch 49/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0119 - accuracy: 0.9957\n",
            "Epoch 50/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0111 - accuracy: 0.9960\n",
            "Epoch 51/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9953\n",
            "Epoch 52/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9954\n",
            "Epoch 53/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9948\n",
            "Epoch 54/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9953\n",
            "Epoch 55/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0111 - accuracy: 0.9962\n",
            "Epoch 56/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9951\n",
            "Epoch 57/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9953\n",
            "Epoch 58/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9943\n",
            "Epoch 59/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0135 - accuracy: 0.9951\n",
            "Epoch 60/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9955\n",
            "14/14 [==============================] - 1s 15ms/step - loss: 0.6729 - accuracy: 0.7967\n",
            "Epoch 1/60\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5154 - accuracy: 0.6964\n",
            "Epoch 2/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3840 - accuracy: 0.7935\n",
            "Epoch 3/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3423 - accuracy: 0.8198\n",
            "Epoch 4/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.3002 - accuracy: 0.8504\n",
            "Epoch 5/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2617 - accuracy: 0.8750\n",
            "Epoch 6/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2279 - accuracy: 0.8936\n",
            "Epoch 7/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.1886 - accuracy: 0.9177\n",
            "Epoch 8/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1551 - accuracy: 0.9329\n",
            "Epoch 9/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1177 - accuracy: 0.9519\n",
            "Epoch 10/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0998 - accuracy: 0.9621\n",
            "Epoch 11/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0811 - accuracy: 0.9680\n",
            "Epoch 12/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0683 - accuracy: 0.9738\n",
            "Epoch 13/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0632 - accuracy: 0.9766\n",
            "Epoch 14/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0459 - accuracy: 0.9847\n",
            "Epoch 15/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0411 - accuracy: 0.9856\n",
            "Epoch 16/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0359 - accuracy: 0.9872\n",
            "Epoch 17/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0281 - accuracy: 0.9906\n",
            "Epoch 18/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0319 - accuracy: 0.9882\n",
            "Epoch 19/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0325 - accuracy: 0.9883\n",
            "Epoch 20/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0292 - accuracy: 0.9898\n",
            "Epoch 21/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0277 - accuracy: 0.9906\n",
            "Epoch 22/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0226 - accuracy: 0.9922\n",
            "Epoch 23/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0216 - accuracy: 0.9922\n",
            "Epoch 24/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 25/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9934\n",
            "Epoch 26/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0194 - accuracy: 0.9938\n",
            "Epoch 27/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0213 - accuracy: 0.9923\n",
            "Epoch 28/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0213 - accuracy: 0.9925\n",
            "Epoch 29/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0216 - accuracy: 0.9925\n",
            "Epoch 30/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0174 - accuracy: 0.9941\n",
            "Epoch 31/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9937\n",
            "Epoch 32/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0154 - accuracy: 0.9940\n",
            "Epoch 33/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0162 - accuracy: 0.9937\n",
            "Epoch 34/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0201 - accuracy: 0.9928\n",
            "Epoch 35/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0222 - accuracy: 0.9917\n",
            "Epoch 36/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0178 - accuracy: 0.9935\n",
            "Epoch 37/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0199 - accuracy: 0.9926\n",
            "Epoch 38/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0186 - accuracy: 0.9935\n",
            "Epoch 39/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9943\n",
            "Epoch 40/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9944\n",
            "Epoch 41/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0166 - accuracy: 0.9942\n",
            "Epoch 42/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9940\n",
            "Epoch 43/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0154 - accuracy: 0.9943\n",
            "Epoch 44/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0162 - accuracy: 0.9948\n",
            "Epoch 45/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9950\n",
            "Epoch 46/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0195 - accuracy: 0.9939\n",
            "Epoch 47/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0129 - accuracy: 0.9951\n",
            "Epoch 48/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9944\n",
            "Epoch 49/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0152 - accuracy: 0.9949\n",
            "Epoch 50/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0134 - accuracy: 0.9952\n",
            "Epoch 51/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9951\n",
            "Epoch 52/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0122 - accuracy: 0.9956\n",
            "Epoch 53/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0136 - accuracy: 0.9954\n",
            "Epoch 54/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0120 - accuracy: 0.9963\n",
            "Epoch 55/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0116 - accuracy: 0.9957\n",
            "Epoch 56/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9954\n",
            "Epoch 57/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0135 - accuracy: 0.9962\n",
            "Epoch 58/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9956\n",
            "Epoch 59/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9948\n",
            "Epoch 60/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0166 - accuracy: 0.9938\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7103 - accuracy: 0.7916\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5161 - accuracy: 0.7040\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3897 - accuracy: 0.7924\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3500 - accuracy: 0.8181\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3104 - accuracy: 0.8436\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2737 - accuracy: 0.8668\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2336 - accuracy: 0.8912\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1988 - accuracy: 0.9094\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1631 - accuracy: 0.9305\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1435 - accuracy: 0.9427\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.1124 - accuracy: 0.9542\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0994 - accuracy: 0.9616\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0823 - accuracy: 0.9680\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0677 - accuracy: 0.9747\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0538 - accuracy: 0.9794\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0497 - accuracy: 0.9817\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0432 - accuracy: 0.9850\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0332 - accuracy: 0.9889\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0366 - accuracy: 0.9869\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0364 - accuracy: 0.9875\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0334 - accuracy: 0.9882\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0280 - accuracy: 0.9898\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0297 - accuracy: 0.9890\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0238 - accuracy: 0.9917\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0248 - accuracy: 0.9914\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0232 - accuracy: 0.9921\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0272 - accuracy: 0.9902\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0236 - accuracy: 0.9913\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0234 - accuracy: 0.9914\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0225 - accuracy: 0.9921\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0178 - accuracy: 0.9937\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0194 - accuracy: 0.9937\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9937\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9939\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0225 - accuracy: 0.9920\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0216 - accuracy: 0.9919\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0207 - accuracy: 0.9929\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0198 - accuracy: 0.9932\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0148 - accuracy: 0.9953\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0145 - accuracy: 0.9945\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9952\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9935\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0172 - accuracy: 0.9941\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0200 - accuracy: 0.9933\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0161 - accuracy: 0.9944\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0131 - accuracy: 0.9952\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0157 - accuracy: 0.9945\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0163 - accuracy: 0.9952\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0170 - accuracy: 0.9945\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0174 - accuracy: 0.9940\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0172 - accuracy: 0.9940\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9948\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9951\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0161 - accuracy: 0.9949\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0121 - accuracy: 0.9956\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0134 - accuracy: 0.9959\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9948\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0141 - accuracy: 0.9946\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0134 - accuracy: 0.9953\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0170 - accuracy: 0.9938\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0156 - accuracy: 0.9944\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9950\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0144 - accuracy: 0.9951\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0119 - accuracy: 0.9963\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0132 - accuracy: 0.9952\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0122 - accuracy: 0.9956\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0107 - accuracy: 0.9965\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0103 - accuracy: 0.9964\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0110 - accuracy: 0.9964\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0119 - accuracy: 0.9957\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0122 - accuracy: 0.9957\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0085 - accuracy: 0.9968\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0107 - accuracy: 0.9963\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0139 - accuracy: 0.9953\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9965\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0145 - accuracy: 0.9950\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0093 - accuracy: 0.9968\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0099 - accuracy: 0.9966\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0110 - accuracy: 0.9965\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0096 - accuracy: 0.9970\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9958\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0114 - accuracy: 0.9966\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0109 - accuracy: 0.9964\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0114 - accuracy: 0.9960\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0110 - accuracy: 0.9966\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0117 - accuracy: 0.9958\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0114 - accuracy: 0.9959\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0135 - accuracy: 0.9955\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0135 - accuracy: 0.9957\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0123 - accuracy: 0.9962\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0111 - accuracy: 0.9968\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0089 - accuracy: 0.9968\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0103 - accuracy: 0.9968\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0069 - accuracy: 0.9973\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0082 - accuracy: 0.9970\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0109 - accuracy: 0.9967\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7800 - accuracy: 0.7930\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5166 - accuracy: 0.7023\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3874 - accuracy: 0.7919\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3545 - accuracy: 0.8150\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3096 - accuracy: 0.8434\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2815 - accuracy: 0.8613\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2377 - accuracy: 0.8861\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2067 - accuracy: 0.9056\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1782 - accuracy: 0.9219\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1448 - accuracy: 0.9397\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1188 - accuracy: 0.9512\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0964 - accuracy: 0.9621\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0750 - accuracy: 0.9714\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0710 - accuracy: 0.9735\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0602 - accuracy: 0.9786\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0491 - accuracy: 0.9820\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0415 - accuracy: 0.9848\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0362 - accuracy: 0.9873\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0312 - accuracy: 0.9887\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0317 - accuracy: 0.9890\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0294 - accuracy: 0.9900\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0277 - accuracy: 0.9902\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0281 - accuracy: 0.9905\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0286 - accuracy: 0.9902\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0275 - accuracy: 0.9908\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0246 - accuracy: 0.9914\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0222 - accuracy: 0.9918\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0210 - accuracy: 0.9927\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0204 - accuracy: 0.9936\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0213 - accuracy: 0.9926\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0228 - accuracy: 0.9914\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0203 - accuracy: 0.9931\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0222 - accuracy: 0.9923\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0196 - accuracy: 0.9930\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0195 - accuracy: 0.9926\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0185 - accuracy: 0.9931\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0185 - accuracy: 0.9935\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0210 - accuracy: 0.9923\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0172 - accuracy: 0.9938\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9954\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9949\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0181 - accuracy: 0.9942\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0197 - accuracy: 0.9937\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0156 - accuracy: 0.9945\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0163 - accuracy: 0.9940\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0145 - accuracy: 0.9948\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0158 - accuracy: 0.9940\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0163 - accuracy: 0.9940\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0168 - accuracy: 0.9946\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0177 - accuracy: 0.9939\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9940\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0139 - accuracy: 0.9950\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9948\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0152 - accuracy: 0.9948\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0135 - accuracy: 0.9953\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0158 - accuracy: 0.9944\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0160 - accuracy: 0.9941\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9952\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0118 - accuracy: 0.9958\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0156 - accuracy: 0.9951\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0137 - accuracy: 0.9950\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0153 - accuracy: 0.9950\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0144 - accuracy: 0.9948\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0120 - accuracy: 0.9953\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9951\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0103 - accuracy: 0.9964\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0121 - accuracy: 0.9959\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0107 - accuracy: 0.9963\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0120 - accuracy: 0.9958\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9965\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0147 - accuracy: 0.9953\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9958\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0121 - accuracy: 0.9956\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9950\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0107 - accuracy: 0.9967\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0081 - accuracy: 0.9973\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0110 - accuracy: 0.9963\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0082 - accuracy: 0.9969\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0108 - accuracy: 0.9967\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0109 - accuracy: 0.9962\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9966\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9956\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9949\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9963\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0114 - accuracy: 0.9958\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0099 - accuracy: 0.9964\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0134 - accuracy: 0.9957\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0124 - accuracy: 0.9960\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0117 - accuracy: 0.9959\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0120 - accuracy: 0.9963\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0104 - accuracy: 0.9966\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0128 - accuracy: 0.9953\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0106 - accuracy: 0.9963\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0095 - accuracy: 0.9964\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0077 - accuracy: 0.9974\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0094 - accuracy: 0.9968\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0098 - accuracy: 0.9972\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0091 - accuracy: 0.9967\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0090 - accuracy: 0.9969\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7861 - accuracy: 0.8014\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5111 - accuracy: 0.7036\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3929 - accuracy: 0.7887\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3461 - accuracy: 0.8196\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3160 - accuracy: 0.8412\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2773 - accuracy: 0.8636\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2340 - accuracy: 0.8901\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1950 - accuracy: 0.9140\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1677 - accuracy: 0.9274\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1286 - accuracy: 0.9469\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1128 - accuracy: 0.9539\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0892 - accuracy: 0.9668\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0733 - accuracy: 0.9711\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0630 - accuracy: 0.9770\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0520 - accuracy: 0.9805\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0479 - accuracy: 0.9838\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0434 - accuracy: 0.9853\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0396 - accuracy: 0.9860\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0346 - accuracy: 0.9875\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0353 - accuracy: 0.9876\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0287 - accuracy: 0.9903\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0278 - accuracy: 0.9901\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0255 - accuracy: 0.9911\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0240 - accuracy: 0.9915\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0233 - accuracy: 0.9922\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0228 - accuracy: 0.9923\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0239 - accuracy: 0.9921\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0249 - accuracy: 0.9908\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0225 - accuracy: 0.9919\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0212 - accuracy: 0.9932\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0237 - accuracy: 0.9907\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0203 - accuracy: 0.9929\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9949\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0170 - accuracy: 0.9948\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0166 - accuracy: 0.9948\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0164 - accuracy: 0.9946\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0178 - accuracy: 0.9939\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9944\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0170 - accuracy: 0.9940\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9952\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0122 - accuracy: 0.9959\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0133 - accuracy: 0.9949\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0163 - accuracy: 0.9950\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0142 - accuracy: 0.9950\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0158 - accuracy: 0.9941\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0171 - accuracy: 0.9943\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0177 - accuracy: 0.9936\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9947\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9954\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0139 - accuracy: 0.9959\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9958\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9937\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9948\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0156 - accuracy: 0.9942\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0144 - accuracy: 0.9951\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0156 - accuracy: 0.9947\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9941\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9956\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0117 - accuracy: 0.9958\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0112 - accuracy: 0.9961\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9957\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9950\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9936\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0170 - accuracy: 0.9939\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9940\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0138 - accuracy: 0.9953\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0151 - accuracy: 0.9948\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9956\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9940\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9952\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0123 - accuracy: 0.9955\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0109 - accuracy: 0.9958\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0137 - accuracy: 0.9954\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9952\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9955\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9953\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9943\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0165 - accuracy: 0.9943\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0142 - accuracy: 0.9948\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0106 - accuracy: 0.9963\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9957\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0111 - accuracy: 0.9966\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9963\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0109 - accuracy: 0.9962\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0135 - accuracy: 0.9956\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9961\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0096 - accuracy: 0.9974\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9961\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0079 - accuracy: 0.9975\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0057 - accuracy: 0.9981\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0086 - accuracy: 0.9970\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0096 - accuracy: 0.9970\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9970\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9957\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9963\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0117 - accuracy: 0.9962\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0093 - accuracy: 0.9967\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9965\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7989 - accuracy: 0.7970\n",
            "Epoch 1/100\n",
            "42/42 [==============================] - 3s 28ms/step - loss: 0.4847 - accuracy: 0.7245\n",
            "Epoch 2/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.3789 - accuracy: 0.7976\n",
            "Epoch 3/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.3408 - accuracy: 0.8227\n",
            "Epoch 4/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.3081 - accuracy: 0.8434\n",
            "Epoch 5/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.2776 - accuracy: 0.8643\n",
            "Epoch 6/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.2504 - accuracy: 0.8784\n",
            "Epoch 7/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.2109 - accuracy: 0.9021\n",
            "Epoch 8/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1799 - accuracy: 0.9178\n",
            "Epoch 9/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1626 - accuracy: 0.9301\n",
            "Epoch 10/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1377 - accuracy: 0.9435\n",
            "Epoch 11/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.1053 - accuracy: 0.9562\n",
            "Epoch 12/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0829 - accuracy: 0.9681\n",
            "Epoch 13/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0681 - accuracy: 0.9739\n",
            "Epoch 14/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0662 - accuracy: 0.9748\n",
            "Epoch 15/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0668 - accuracy: 0.9737\n",
            "Epoch 16/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0535 - accuracy: 0.9802\n",
            "Epoch 17/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0462 - accuracy: 0.9831\n",
            "Epoch 18/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0379 - accuracy: 0.9864\n",
            "Epoch 19/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0363 - accuracy: 0.9870\n",
            "Epoch 20/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0287 - accuracy: 0.9898\n",
            "Epoch 21/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0316 - accuracy: 0.9886\n",
            "Epoch 22/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0381 - accuracy: 0.9864\n",
            "Epoch 23/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0345 - accuracy: 0.9870\n",
            "Epoch 24/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0296 - accuracy: 0.9890\n",
            "Epoch 25/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0327 - accuracy: 0.9889\n",
            "Epoch 26/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0284 - accuracy: 0.9904\n",
            "Epoch 27/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0234 - accuracy: 0.9919\n",
            "Epoch 28/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0243 - accuracy: 0.9915\n",
            "Epoch 29/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0277 - accuracy: 0.9906\n",
            "Epoch 30/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0263 - accuracy: 0.9911\n",
            "Epoch 31/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0226 - accuracy: 0.9912\n",
            "Epoch 32/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0211 - accuracy: 0.9924\n",
            "Epoch 33/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0186 - accuracy: 0.9932\n",
            "Epoch 34/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0187 - accuracy: 0.9938\n",
            "Epoch 35/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0178 - accuracy: 0.9938\n",
            "Epoch 36/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0172 - accuracy: 0.9936\n",
            "Epoch 37/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0180 - accuracy: 0.9933\n",
            "Epoch 38/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0199 - accuracy: 0.9928\n",
            "Epoch 39/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0241 - accuracy: 0.9917\n",
            "Epoch 40/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0232 - accuracy: 0.9919\n",
            "Epoch 41/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0228 - accuracy: 0.9923\n",
            "Epoch 42/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0258 - accuracy: 0.9922\n",
            "Epoch 43/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0223 - accuracy: 0.9927\n",
            "Epoch 44/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0223 - accuracy: 0.9925\n",
            "Epoch 45/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0203 - accuracy: 0.9932\n",
            "Epoch 46/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0181 - accuracy: 0.9938\n",
            "Epoch 47/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0144 - accuracy: 0.9952\n",
            "Epoch 48/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0166 - accuracy: 0.9945\n",
            "Epoch 49/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9935\n",
            "Epoch 50/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0207 - accuracy: 0.9932\n",
            "Epoch 51/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0211 - accuracy: 0.9932\n",
            "Epoch 52/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0160 - accuracy: 0.9942\n",
            "Epoch 53/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0171 - accuracy: 0.9940\n",
            "Epoch 54/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9962\n",
            "Epoch 55/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0116 - accuracy: 0.9961\n",
            "Epoch 56/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9954\n",
            "Epoch 57/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9946\n",
            "Epoch 58/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9952\n",
            "Epoch 59/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0170 - accuracy: 0.9941\n",
            "Epoch 60/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9940\n",
            "Epoch 61/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0156 - accuracy: 0.9952\n",
            "Epoch 62/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0166 - accuracy: 0.9946\n",
            "Epoch 63/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0149 - accuracy: 0.9945\n",
            "Epoch 64/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9949\n",
            "Epoch 65/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9950\n",
            "Epoch 66/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9953\n",
            "Epoch 67/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 68/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0185 - accuracy: 0.9939\n",
            "Epoch 69/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0160 - accuracy: 0.9942\n",
            "Epoch 70/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0142 - accuracy: 0.9954\n",
            "Epoch 71/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9955\n",
            "Epoch 72/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0148 - accuracy: 0.9954\n",
            "Epoch 73/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9947\n",
            "Epoch 74/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9956\n",
            "Epoch 75/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0090 - accuracy: 0.9967\n",
            "Epoch 76/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0084 - accuracy: 0.9969\n",
            "Epoch 77/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0110 - accuracy: 0.9963\n",
            "Epoch 78/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0116 - accuracy: 0.9962\n",
            "Epoch 79/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9950\n",
            "Epoch 80/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 81/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0156 - accuracy: 0.9945\n",
            "Epoch 82/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0165 - accuracy: 0.9944\n",
            "Epoch 83/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9954\n",
            "Epoch 84/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0104 - accuracy: 0.9963\n",
            "Epoch 85/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0102 - accuracy: 0.9966\n",
            "Epoch 86/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0137 - accuracy: 0.9956\n",
            "Epoch 87/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0109 - accuracy: 0.9961\n",
            "Epoch 88/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9965\n",
            "Epoch 89/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0128 - accuracy: 0.9961\n",
            "Epoch 90/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9963\n",
            "Epoch 91/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 92/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0099 - accuracy: 0.9966\n",
            "Epoch 93/100\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Epoch 94/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0122 - accuracy: 0.9959\n",
            "Epoch 95/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0139 - accuracy: 0.9952\n",
            "Epoch 96/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0175 - accuracy: 0.9946\n",
            "Epoch 97/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0182 - accuracy: 0.9943\n",
            "Epoch 98/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0148 - accuracy: 0.9951\n",
            "Epoch 99/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0105 - accuracy: 0.9963\n",
            "Epoch 100/100\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0115 - accuracy: 0.9965\n",
            "5/5 [==============================] - 0s 7ms/step\n",
            ">acc=0.216, pres=0.813, rec=0.816, f1=0.814, est=0.797, cfg={'batch_size': 100, 'epochs': 100}\n",
            "Epoch 1/20\n",
            "138/138 [==============================] - 4s 11ms/step - loss: 0.4873 - accuracy: 0.7207\n",
            "Epoch 2/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3981 - accuracy: 0.7831\n",
            "Epoch 3/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3559 - accuracy: 0.8082\n",
            "Epoch 4/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3224 - accuracy: 0.8324\n",
            "Epoch 5/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2994 - accuracy: 0.8519\n",
            "Epoch 6/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2641 - accuracy: 0.8729\n",
            "Epoch 7/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2210 - accuracy: 0.8955\n",
            "Epoch 8/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1930 - accuracy: 0.9138\n",
            "Epoch 9/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1607 - accuracy: 0.9282\n",
            "Epoch 10/20\n",
            "138/138 [==============================] - 1s 11ms/step - loss: 0.1374 - accuracy: 0.9402\n",
            "Epoch 11/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1175 - accuracy: 0.9530\n",
            "Epoch 12/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1083 - accuracy: 0.9542\n",
            "Epoch 13/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0949 - accuracy: 0.9638\n",
            "Epoch 14/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0709 - accuracy: 0.9726\n",
            "Epoch 15/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0623 - accuracy: 0.9767\n",
            "Epoch 16/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0601 - accuracy: 0.9782\n",
            "Epoch 17/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0599 - accuracy: 0.9783\n",
            "Epoch 18/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0517 - accuracy: 0.9805\n",
            "Epoch 19/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0450 - accuracy: 0.9836\n",
            "Epoch 20/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0399 - accuracy: 0.9856\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.5509 - accuracy: 0.8010\n",
            "Epoch 1/20\n",
            "138/138 [==============================] - 4s 11ms/step - loss: 0.4849 - accuracy: 0.7228\n",
            "Epoch 2/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3930 - accuracy: 0.7897\n",
            "Epoch 3/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3528 - accuracy: 0.8146\n",
            "Epoch 4/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3228 - accuracy: 0.8333\n",
            "Epoch 5/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2878 - accuracy: 0.8573\n",
            "Epoch 6/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2575 - accuracy: 0.8743\n",
            "Epoch 7/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2251 - accuracy: 0.8932\n",
            "Epoch 8/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1942 - accuracy: 0.9133\n",
            "Epoch 9/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1567 - accuracy: 0.9322\n",
            "Epoch 10/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1370 - accuracy: 0.9414\n",
            "Epoch 11/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1140 - accuracy: 0.9511\n",
            "Epoch 12/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0932 - accuracy: 0.9629\n",
            "Epoch 13/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0831 - accuracy: 0.9684\n",
            "Epoch 14/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0753 - accuracy: 0.9691\n",
            "Epoch 15/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0714 - accuracy: 0.9731\n",
            "Epoch 16/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0602 - accuracy: 0.9773\n",
            "Epoch 17/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0568 - accuracy: 0.9798\n",
            "Epoch 18/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0508 - accuracy: 0.9813\n",
            "Epoch 19/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0504 - accuracy: 0.9824\n",
            "Epoch 20/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0514 - accuracy: 0.9819\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.5797 - accuracy: 0.7891\n",
            "Epoch 1/20\n",
            "138/138 [==============================] - 4s 11ms/step - loss: 0.4795 - accuracy: 0.7301\n",
            "Epoch 2/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3934 - accuracy: 0.7885\n",
            "Epoch 3/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3539 - accuracy: 0.8139\n",
            "Epoch 4/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3219 - accuracy: 0.8365\n",
            "Epoch 5/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2929 - accuracy: 0.8529\n",
            "Epoch 6/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2556 - accuracy: 0.8773\n",
            "Epoch 7/20\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.2288 - accuracy: 0.8934\n",
            "Epoch 8/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1888 - accuracy: 0.9130\n",
            "Epoch 9/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1589 - accuracy: 0.9301\n",
            "Epoch 10/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1370 - accuracy: 0.9435\n",
            "Epoch 11/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1211 - accuracy: 0.9517\n",
            "Epoch 12/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1036 - accuracy: 0.9590\n",
            "Epoch 13/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0858 - accuracy: 0.9657\n",
            "Epoch 14/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0785 - accuracy: 0.9692\n",
            "Epoch 15/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0626 - accuracy: 0.9764\n",
            "Epoch 16/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0623 - accuracy: 0.9758\n",
            "Epoch 17/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0516 - accuracy: 0.9804\n",
            "Epoch 18/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0491 - accuracy: 0.9819\n",
            "Epoch 19/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0505 - accuracy: 0.9815\n",
            "Epoch 20/20\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0526 - accuracy: 0.9801\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.5676 - accuracy: 0.7943\n",
            "Epoch 1/60\n",
            "138/138 [==============================] - 4s 11ms/step - loss: 0.4848 - accuracy: 0.7224\n",
            "Epoch 2/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3951 - accuracy: 0.7896\n",
            "Epoch 3/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3587 - accuracy: 0.8120\n",
            "Epoch 4/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3282 - accuracy: 0.8308\n",
            "Epoch 5/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2942 - accuracy: 0.8515\n",
            "Epoch 6/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2633 - accuracy: 0.8750\n",
            "Epoch 7/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2221 - accuracy: 0.8968\n",
            "Epoch 8/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1970 - accuracy: 0.9101\n",
            "Epoch 9/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1683 - accuracy: 0.9254\n",
            "Epoch 10/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1467 - accuracy: 0.9373\n",
            "Epoch 11/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1179 - accuracy: 0.9504\n",
            "Epoch 12/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0999 - accuracy: 0.9598\n",
            "Epoch 13/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0948 - accuracy: 0.9632\n",
            "Epoch 14/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0813 - accuracy: 0.9683\n",
            "Epoch 15/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0681 - accuracy: 0.9733\n",
            "Epoch 16/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0546 - accuracy: 0.9792\n",
            "Epoch 17/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0565 - accuracy: 0.9782\n",
            "Epoch 18/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0561 - accuracy: 0.9790\n",
            "Epoch 19/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0425 - accuracy: 0.9848\n",
            "Epoch 20/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0409 - accuracy: 0.9853\n",
            "Epoch 21/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0440 - accuracy: 0.9840\n",
            "Epoch 22/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0411 - accuracy: 0.9850\n",
            "Epoch 23/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0401 - accuracy: 0.9857\n",
            "Epoch 24/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0428 - accuracy: 0.9845\n",
            "Epoch 25/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0334 - accuracy: 0.9887\n",
            "Epoch 26/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0387 - accuracy: 0.9867\n",
            "Epoch 27/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0329 - accuracy: 0.9887\n",
            "Epoch 28/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0241 - accuracy: 0.9919\n",
            "Epoch 29/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0331 - accuracy: 0.9875\n",
            "Epoch 30/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0357 - accuracy: 0.9868\n",
            "Epoch 31/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0315 - accuracy: 0.9893\n",
            "Epoch 32/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0326 - accuracy: 0.9891\n",
            "Epoch 33/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0326 - accuracy: 0.9896\n",
            "Epoch 34/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0234 - accuracy: 0.9909\n",
            "Epoch 35/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0299 - accuracy: 0.9910\n",
            "Epoch 36/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0293 - accuracy: 0.9895\n",
            "Epoch 37/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0297 - accuracy: 0.9900\n",
            "Epoch 38/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0269 - accuracy: 0.9913\n",
            "Epoch 39/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0257 - accuracy: 0.9914\n",
            "Epoch 40/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0243 - accuracy: 0.9916\n",
            "Epoch 41/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0225 - accuracy: 0.9925\n",
            "Epoch 42/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0212 - accuracy: 0.9927\n",
            "Epoch 43/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0155 - accuracy: 0.9949\n",
            "Epoch 44/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0192 - accuracy: 0.9931\n",
            "Epoch 45/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0263 - accuracy: 0.9924\n",
            "Epoch 46/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0246 - accuracy: 0.9921\n",
            "Epoch 47/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0259 - accuracy: 0.9916\n",
            "Epoch 48/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0206 - accuracy: 0.9932\n",
            "Epoch 49/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0215 - accuracy: 0.9931\n",
            "Epoch 50/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0246 - accuracy: 0.9920\n",
            "Epoch 51/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0254 - accuracy: 0.9911\n",
            "Epoch 52/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0291 - accuracy: 0.9906\n",
            "Epoch 53/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0214 - accuracy: 0.9937\n",
            "Epoch 54/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0195 - accuracy: 0.9937\n",
            "Epoch 55/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0245 - accuracy: 0.9925\n",
            "Epoch 56/60\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0229 - accuracy: 0.9927\n",
            "Epoch 57/60\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0179 - accuracy: 0.9946\n",
            "Epoch 58/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0164 - accuracy: 0.9942\n",
            "Epoch 59/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0222 - accuracy: 0.9930\n",
            "Epoch 60/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0192 - accuracy: 0.9933\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.7860 - accuracy: 0.7969\n",
            "Epoch 1/60\n",
            "138/138 [==============================] - 4s 11ms/step - loss: 0.4904 - accuracy: 0.7184\n",
            "Epoch 2/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3913 - accuracy: 0.7866\n",
            "Epoch 3/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3537 - accuracy: 0.8171\n",
            "Epoch 4/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3208 - accuracy: 0.8349\n",
            "Epoch 5/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2862 - accuracy: 0.8583\n",
            "Epoch 6/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2551 - accuracy: 0.8761\n",
            "Epoch 7/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2157 - accuracy: 0.8994\n",
            "Epoch 8/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1953 - accuracy: 0.9117\n",
            "Epoch 9/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1630 - accuracy: 0.9284\n",
            "Epoch 10/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1276 - accuracy: 0.9448\n",
            "Epoch 11/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1101 - accuracy: 0.9564\n",
            "Epoch 12/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1017 - accuracy: 0.9602\n",
            "Epoch 13/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0807 - accuracy: 0.9669\n",
            "Epoch 14/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0706 - accuracy: 0.9740\n",
            "Epoch 15/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0635 - accuracy: 0.9768\n",
            "Epoch 16/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0575 - accuracy: 0.9787\n",
            "Epoch 17/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0587 - accuracy: 0.9787\n",
            "Epoch 18/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0547 - accuracy: 0.9806\n",
            "Epoch 19/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0567 - accuracy: 0.9793\n",
            "Epoch 20/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0442 - accuracy: 0.9844\n",
            "Epoch 21/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0421 - accuracy: 0.9841\n",
            "Epoch 22/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0456 - accuracy: 0.9839\n",
            "Epoch 23/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0388 - accuracy: 0.9857\n",
            "Epoch 24/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0366 - accuracy: 0.9861\n",
            "Epoch 25/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0324 - accuracy: 0.9885\n",
            "Epoch 26/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0391 - accuracy: 0.9865\n",
            "Epoch 27/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0369 - accuracy: 0.9878\n",
            "Epoch 28/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0310 - accuracy: 0.9895\n",
            "Epoch 29/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0295 - accuracy: 0.9897\n",
            "Epoch 30/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0252 - accuracy: 0.9908\n",
            "Epoch 31/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0255 - accuracy: 0.9917\n",
            "Epoch 32/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0341 - accuracy: 0.9890\n",
            "Epoch 33/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0267 - accuracy: 0.9906\n",
            "Epoch 34/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0300 - accuracy: 0.9901\n",
            "Epoch 35/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0278 - accuracy: 0.9905\n",
            "Epoch 36/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0223 - accuracy: 0.9929\n",
            "Epoch 37/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0305 - accuracy: 0.9898\n",
            "Epoch 38/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0302 - accuracy: 0.9902\n",
            "Epoch 39/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0285 - accuracy: 0.9901\n",
            "Epoch 40/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0263 - accuracy: 0.9909\n",
            "Epoch 41/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0289 - accuracy: 0.9909\n",
            "Epoch 42/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0257 - accuracy: 0.9918\n",
            "Epoch 43/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0240 - accuracy: 0.9916\n",
            "Epoch 44/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0179 - accuracy: 0.9931\n",
            "Epoch 45/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0225 - accuracy: 0.9922\n",
            "Epoch 46/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0199 - accuracy: 0.9928\n",
            "Epoch 47/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0194 - accuracy: 0.9935\n",
            "Epoch 48/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0179 - accuracy: 0.9945\n",
            "Epoch 49/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0229 - accuracy: 0.9935\n",
            "Epoch 50/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0219 - accuracy: 0.9926\n",
            "Epoch 51/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0192 - accuracy: 0.9935\n",
            "Epoch 52/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0187 - accuracy: 0.9938\n",
            "Epoch 53/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0200 - accuracy: 0.9937\n",
            "Epoch 54/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0259 - accuracy: 0.9913\n",
            "Epoch 55/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0245 - accuracy: 0.9915\n",
            "Epoch 56/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0205 - accuracy: 0.9925\n",
            "Epoch 57/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0174 - accuracy: 0.9941\n",
            "Epoch 58/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0185 - accuracy: 0.9944\n",
            "Epoch 59/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0236 - accuracy: 0.9929\n",
            "Epoch 60/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0214 - accuracy: 0.9931\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.8277 - accuracy: 0.7977\n",
            "Epoch 1/60\n",
            "138/138 [==============================] - 4s 11ms/step - loss: 0.4879 - accuracy: 0.7206\n",
            "Epoch 2/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3914 - accuracy: 0.7904\n",
            "Epoch 3/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3607 - accuracy: 0.8098\n",
            "Epoch 4/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3289 - accuracy: 0.8315\n",
            "Epoch 5/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2992 - accuracy: 0.8490\n",
            "Epoch 6/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2587 - accuracy: 0.8761\n",
            "Epoch 7/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2342 - accuracy: 0.8882\n",
            "Epoch 8/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1976 - accuracy: 0.9102\n",
            "Epoch 9/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1677 - accuracy: 0.9261\n",
            "Epoch 10/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1505 - accuracy: 0.9352\n",
            "Epoch 11/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1306 - accuracy: 0.9438\n",
            "Epoch 12/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1078 - accuracy: 0.9567\n",
            "Epoch 13/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0837 - accuracy: 0.9662\n",
            "Epoch 14/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0769 - accuracy: 0.9710\n",
            "Epoch 15/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0712 - accuracy: 0.9714\n",
            "Epoch 16/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0589 - accuracy: 0.9779\n",
            "Epoch 17/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0663 - accuracy: 0.9747\n",
            "Epoch 18/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0606 - accuracy: 0.9762\n",
            "Epoch 19/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0504 - accuracy: 0.9805\n",
            "Epoch 20/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0456 - accuracy: 0.9834\n",
            "Epoch 21/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0381 - accuracy: 0.9859\n",
            "Epoch 22/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0425 - accuracy: 0.9844\n",
            "Epoch 23/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0375 - accuracy: 0.9869\n",
            "Epoch 24/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0377 - accuracy: 0.9871\n",
            "Epoch 25/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0354 - accuracy: 0.9869\n",
            "Epoch 26/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0328 - accuracy: 0.9888\n",
            "Epoch 27/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0358 - accuracy: 0.9865\n",
            "Epoch 28/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0395 - accuracy: 0.9860\n",
            "Epoch 29/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0409 - accuracy: 0.9854\n",
            "Epoch 30/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0325 - accuracy: 0.9888\n",
            "Epoch 31/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0276 - accuracy: 0.9910\n",
            "Epoch 32/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0303 - accuracy: 0.9892\n",
            "Epoch 33/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0304 - accuracy: 0.9904\n",
            "Epoch 34/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0301 - accuracy: 0.9894\n",
            "Epoch 35/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0323 - accuracy: 0.9896\n",
            "Epoch 36/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0303 - accuracy: 0.9897\n",
            "Epoch 37/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0277 - accuracy: 0.9905\n",
            "Epoch 38/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0273 - accuracy: 0.9904\n",
            "Epoch 39/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0330 - accuracy: 0.9890\n",
            "Epoch 40/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0299 - accuracy: 0.9911\n",
            "Epoch 41/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0271 - accuracy: 0.9911\n",
            "Epoch 42/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0237 - accuracy: 0.9928\n",
            "Epoch 43/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0267 - accuracy: 0.9912\n",
            "Epoch 44/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0235 - accuracy: 0.9917\n",
            "Epoch 45/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0222 - accuracy: 0.9928\n",
            "Epoch 46/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0197 - accuracy: 0.9932\n",
            "Epoch 47/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0213 - accuracy: 0.9921\n",
            "Epoch 48/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0182 - accuracy: 0.9940\n",
            "Epoch 49/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0228 - accuracy: 0.9941\n",
            "Epoch 50/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0190 - accuracy: 0.9937\n",
            "Epoch 51/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0231 - accuracy: 0.9925\n",
            "Epoch 52/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0219 - accuracy: 0.9930\n",
            "Epoch 53/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0221 - accuracy: 0.9925\n",
            "Epoch 54/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0268 - accuracy: 0.9909\n",
            "Epoch 55/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0236 - accuracy: 0.9934\n",
            "Epoch 56/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0206 - accuracy: 0.9944\n",
            "Epoch 57/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0124 - accuracy: 0.9959\n",
            "Epoch 58/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0172 - accuracy: 0.9949\n",
            "Epoch 59/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0142 - accuracy: 0.9952\n",
            "Epoch 60/60\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0235 - accuracy: 0.9935\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.8504 - accuracy: 0.7931\n",
            "Epoch 1/100\n",
            "138/138 [==============================] - 3s 11ms/step - loss: 0.4863 - accuracy: 0.7218\n",
            "Epoch 2/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3931 - accuracy: 0.7866\n",
            "Epoch 3/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3579 - accuracy: 0.8115\n",
            "Epoch 4/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3243 - accuracy: 0.8323\n",
            "Epoch 5/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2966 - accuracy: 0.8476\n",
            "Epoch 6/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2518 - accuracy: 0.8756\n",
            "Epoch 7/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2229 - accuracy: 0.8930\n",
            "Epoch 8/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1935 - accuracy: 0.9120\n",
            "Epoch 9/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1651 - accuracy: 0.9288\n",
            "Epoch 10/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1414 - accuracy: 0.9400\n",
            "Epoch 11/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1178 - accuracy: 0.9515\n",
            "Epoch 12/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1032 - accuracy: 0.9588\n",
            "Epoch 13/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0831 - accuracy: 0.9683\n",
            "Epoch 14/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0814 - accuracy: 0.9685\n",
            "Epoch 15/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0682 - accuracy: 0.9726\n",
            "Epoch 16/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0630 - accuracy: 0.9764\n",
            "Epoch 17/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0587 - accuracy: 0.9786\n",
            "Epoch 18/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0553 - accuracy: 0.9793\n",
            "Epoch 19/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0515 - accuracy: 0.9810\n",
            "Epoch 20/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0445 - accuracy: 0.9847\n",
            "Epoch 21/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0364 - accuracy: 0.9860\n",
            "Epoch 22/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0415 - accuracy: 0.9863\n",
            "Epoch 23/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0402 - accuracy: 0.9849\n",
            "Epoch 24/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0382 - accuracy: 0.9868\n",
            "Epoch 25/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0432 - accuracy: 0.9853\n",
            "Epoch 26/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0305 - accuracy: 0.9895\n",
            "Epoch 27/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0353 - accuracy: 0.9876\n",
            "Epoch 28/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0291 - accuracy: 0.9905\n",
            "Epoch 29/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0323 - accuracy: 0.9886\n",
            "Epoch 30/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0326 - accuracy: 0.9872\n",
            "Epoch 31/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0368 - accuracy: 0.9874\n",
            "Epoch 32/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0315 - accuracy: 0.9890\n",
            "Epoch 33/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0293 - accuracy: 0.9899\n",
            "Epoch 34/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0348 - accuracy: 0.9883\n",
            "Epoch 35/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0279 - accuracy: 0.9909\n",
            "Epoch 36/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0281 - accuracy: 0.9913\n",
            "Epoch 37/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0309 - accuracy: 0.9899\n",
            "Epoch 38/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0286 - accuracy: 0.9919\n",
            "Epoch 39/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0283 - accuracy: 0.9898\n",
            "Epoch 40/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0298 - accuracy: 0.9894\n",
            "Epoch 41/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0254 - accuracy: 0.9911\n",
            "Epoch 42/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0281 - accuracy: 0.9902\n",
            "Epoch 43/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0270 - accuracy: 0.9908\n",
            "Epoch 44/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0227 - accuracy: 0.9919\n",
            "Epoch 45/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0220 - accuracy: 0.9928\n",
            "Epoch 46/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0241 - accuracy: 0.9921\n",
            "Epoch 47/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0209 - accuracy: 0.9928\n",
            "Epoch 48/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0266 - accuracy: 0.9914\n",
            "Epoch 49/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0267 - accuracy: 0.9909\n",
            "Epoch 50/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0224 - accuracy: 0.9930\n",
            "Epoch 51/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0290 - accuracy: 0.9913\n",
            "Epoch 52/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0197 - accuracy: 0.9931\n",
            "Epoch 53/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0210 - accuracy: 0.9935\n",
            "Epoch 54/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0171 - accuracy: 0.9950\n",
            "Epoch 55/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0180 - accuracy: 0.9945\n",
            "Epoch 56/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0198 - accuracy: 0.9942\n",
            "Epoch 57/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0194 - accuracy: 0.9940\n",
            "Epoch 58/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0154 - accuracy: 0.9955\n",
            "Epoch 59/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0157 - accuracy: 0.9948\n",
            "Epoch 60/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0218 - accuracy: 0.9928\n",
            "Epoch 61/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0178 - accuracy: 0.9949\n",
            "Epoch 62/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0118 - accuracy: 0.9957\n",
            "Epoch 63/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0233 - accuracy: 0.9937\n",
            "Epoch 64/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0130 - accuracy: 0.9950\n",
            "Epoch 65/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0171 - accuracy: 0.9944\n",
            "Epoch 66/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0196 - accuracy: 0.9938\n",
            "Epoch 67/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0220 - accuracy: 0.9933\n",
            "Epoch 68/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0249 - accuracy: 0.9924\n",
            "Epoch 69/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0204 - accuracy: 0.9936\n",
            "Epoch 70/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0162 - accuracy: 0.9952\n",
            "Epoch 71/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0195 - accuracy: 0.9943\n",
            "Epoch 72/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0120 - accuracy: 0.9954\n",
            "Epoch 73/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0149 - accuracy: 0.9954\n",
            "Epoch 74/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0194 - accuracy: 0.9938\n",
            "Epoch 75/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0160 - accuracy: 0.9952\n",
            "Epoch 76/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0119 - accuracy: 0.9964\n",
            "Epoch 77/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0199 - accuracy: 0.9933\n",
            "Epoch 78/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0187 - accuracy: 0.9937\n",
            "Epoch 79/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0207 - accuracy: 0.9939\n",
            "Epoch 80/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0161 - accuracy: 0.9950\n",
            "Epoch 81/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0174 - accuracy: 0.9949\n",
            "Epoch 82/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0132 - accuracy: 0.9958\n",
            "Epoch 83/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0163 - accuracy: 0.9945\n",
            "Epoch 84/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0152 - accuracy: 0.9948\n",
            "Epoch 85/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0138 - accuracy: 0.9952\n",
            "Epoch 86/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0143 - accuracy: 0.9959\n",
            "Epoch 87/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0154 - accuracy: 0.9950\n",
            "Epoch 88/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0118 - accuracy: 0.9964\n",
            "Epoch 89/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0157 - accuracy: 0.9954\n",
            "Epoch 90/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0121 - accuracy: 0.9956\n",
            "Epoch 91/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0153 - accuracy: 0.9955\n",
            "Epoch 92/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0135 - accuracy: 0.9959\n",
            "Epoch 93/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0116 - accuracy: 0.9969\n",
            "Epoch 94/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0083 - accuracy: 0.9972\n",
            "Epoch 95/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0067 - accuracy: 0.9977\n",
            "Epoch 96/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0094 - accuracy: 0.9972\n",
            "Epoch 97/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0135 - accuracy: 0.9956\n",
            "Epoch 98/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0147 - accuracy: 0.9954\n",
            "Epoch 99/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0159 - accuracy: 0.9954\n",
            "Epoch 100/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0185 - accuracy: 0.9942\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.9269 - accuracy: 0.7992\n",
            "Epoch 1/100\n",
            "138/138 [==============================] - 3s 11ms/step - loss: 0.4930 - accuracy: 0.7183\n",
            "Epoch 2/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3950 - accuracy: 0.7887\n",
            "Epoch 3/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3530 - accuracy: 0.8145\n",
            "Epoch 4/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3258 - accuracy: 0.8326\n",
            "Epoch 5/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2907 - accuracy: 0.8557\n",
            "Epoch 6/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2566 - accuracy: 0.8726\n",
            "Epoch 7/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2229 - accuracy: 0.8961\n",
            "Epoch 8/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1997 - accuracy: 0.9093\n",
            "Epoch 9/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1760 - accuracy: 0.9193\n",
            "Epoch 10/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1402 - accuracy: 0.9412\n",
            "Epoch 11/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1187 - accuracy: 0.9515\n",
            "Epoch 12/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0943 - accuracy: 0.9642\n",
            "Epoch 13/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0890 - accuracy: 0.9642\n",
            "Epoch 14/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0767 - accuracy: 0.9713\n",
            "Epoch 15/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0626 - accuracy: 0.9765\n",
            "Epoch 16/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0617 - accuracy: 0.9775\n",
            "Epoch 17/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0508 - accuracy: 0.9817\n",
            "Epoch 18/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0519 - accuracy: 0.9806\n",
            "Epoch 19/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0490 - accuracy: 0.9818\n",
            "Epoch 20/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0421 - accuracy: 0.9847\n",
            "Epoch 21/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0414 - accuracy: 0.9848\n",
            "Epoch 22/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0466 - accuracy: 0.9834\n",
            "Epoch 23/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0382 - accuracy: 0.9867\n",
            "Epoch 24/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0418 - accuracy: 0.9854\n",
            "Epoch 25/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0383 - accuracy: 0.9859\n",
            "Epoch 26/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0322 - accuracy: 0.9882\n",
            "Epoch 27/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0274 - accuracy: 0.9909\n",
            "Epoch 28/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0284 - accuracy: 0.9901\n",
            "Epoch 29/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0288 - accuracy: 0.9899\n",
            "Epoch 30/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0327 - accuracy: 0.9887\n",
            "Epoch 31/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0390 - accuracy: 0.9864\n",
            "Epoch 32/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0402 - accuracy: 0.9858\n",
            "Epoch 33/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0330 - accuracy: 0.9884\n",
            "Epoch 34/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0304 - accuracy: 0.9890\n",
            "Epoch 35/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0306 - accuracy: 0.9909\n",
            "Epoch 36/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0270 - accuracy: 0.9915\n",
            "Epoch 37/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0254 - accuracy: 0.9912\n",
            "Epoch 38/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0269 - accuracy: 0.9915\n",
            "Epoch 39/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0190 - accuracy: 0.9936\n",
            "Epoch 40/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0248 - accuracy: 0.9913\n",
            "Epoch 41/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0252 - accuracy: 0.9917\n",
            "Epoch 42/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0217 - accuracy: 0.9924\n",
            "Epoch 43/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0243 - accuracy: 0.9917\n",
            "Epoch 44/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0259 - accuracy: 0.9915\n",
            "Epoch 45/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0326 - accuracy: 0.9887\n",
            "Epoch 46/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0255 - accuracy: 0.9912\n",
            "Epoch 47/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0269 - accuracy: 0.9915\n",
            "Epoch 48/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0232 - accuracy: 0.9921\n",
            "Epoch 49/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0185 - accuracy: 0.9934\n",
            "Epoch 50/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0197 - accuracy: 0.9939\n",
            "Epoch 51/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0171 - accuracy: 0.9941\n",
            "Epoch 52/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0169 - accuracy: 0.9940\n",
            "Epoch 53/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0133 - accuracy: 0.9945\n",
            "Epoch 54/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0173 - accuracy: 0.9948\n",
            "Epoch 55/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0193 - accuracy: 0.9938\n",
            "Epoch 56/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0244 - accuracy: 0.9927\n",
            "Epoch 57/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0284 - accuracy: 0.9919\n",
            "Epoch 58/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0210 - accuracy: 0.9924\n",
            "Epoch 59/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0202 - accuracy: 0.9931\n",
            "Epoch 60/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0219 - accuracy: 0.9938\n",
            "Epoch 61/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0155 - accuracy: 0.9948\n",
            "Epoch 62/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 63/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0246 - accuracy: 0.9925\n",
            "Epoch 64/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0210 - accuracy: 0.9925\n",
            "Epoch 65/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0179 - accuracy: 0.9935\n",
            "Epoch 66/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0199 - accuracy: 0.9939\n",
            "Epoch 67/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0183 - accuracy: 0.9943\n",
            "Epoch 68/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0119 - accuracy: 0.9963\n",
            "Epoch 69/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0136 - accuracy: 0.9961\n",
            "Epoch 70/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0145 - accuracy: 0.9947\n",
            "Epoch 71/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0173 - accuracy: 0.9946\n",
            "Epoch 72/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0167 - accuracy: 0.9948\n",
            "Epoch 73/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0178 - accuracy: 0.9947\n",
            "Epoch 74/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0154 - accuracy: 0.9948\n",
            "Epoch 75/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0138 - accuracy: 0.9958\n",
            "Epoch 76/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0178 - accuracy: 0.9943\n",
            "Epoch 77/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 78/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0169 - accuracy: 0.9949\n",
            "Epoch 79/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0166 - accuracy: 0.9946\n",
            "Epoch 80/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0186 - accuracy: 0.9952\n",
            "Epoch 81/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0161 - accuracy: 0.9957\n",
            "Epoch 82/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0122 - accuracy: 0.9961\n",
            "Epoch 83/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0133 - accuracy: 0.9950\n",
            "Epoch 84/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 85/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0144 - accuracy: 0.9955\n",
            "Epoch 86/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0147 - accuracy: 0.9956\n",
            "Epoch 87/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0151 - accuracy: 0.9956\n",
            "Epoch 88/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0126 - accuracy: 0.9959\n",
            "Epoch 89/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0156 - accuracy: 0.9949\n",
            "Epoch 90/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0122 - accuracy: 0.9965\n",
            "Epoch 91/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0129 - accuracy: 0.9960\n",
            "Epoch 92/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0106 - accuracy: 0.9971\n",
            "Epoch 93/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0158 - accuracy: 0.9961\n",
            "Epoch 94/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0125 - accuracy: 0.9966\n",
            "Epoch 95/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0136 - accuracy: 0.9961\n",
            "Epoch 96/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0104 - accuracy: 0.9963\n",
            "Epoch 97/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0139 - accuracy: 0.9955\n",
            "Epoch 98/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0147 - accuracy: 0.9955\n",
            "Epoch 99/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0182 - accuracy: 0.9945\n",
            "Epoch 100/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0104 - accuracy: 0.9970\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.9274 - accuracy: 0.7948\n",
            "Epoch 1/100\n",
            "138/138 [==============================] - 3s 11ms/step - loss: 0.4835 - accuracy: 0.7255\n",
            "Epoch 2/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3932 - accuracy: 0.7896\n",
            "Epoch 3/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3535 - accuracy: 0.8137\n",
            "Epoch 4/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.3248 - accuracy: 0.8351\n",
            "Epoch 5/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2944 - accuracy: 0.8527\n",
            "Epoch 6/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2626 - accuracy: 0.8738\n",
            "Epoch 7/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.2260 - accuracy: 0.8947\n",
            "Epoch 8/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1991 - accuracy: 0.9104\n",
            "Epoch 9/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.1677 - accuracy: 0.9266\n",
            "Epoch 10/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1426 - accuracy: 0.9379\n",
            "Epoch 11/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1161 - accuracy: 0.9523\n",
            "Epoch 12/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.1017 - accuracy: 0.9601\n",
            "Epoch 13/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0892 - accuracy: 0.9648\n",
            "Epoch 14/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0732 - accuracy: 0.9724\n",
            "Epoch 15/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0668 - accuracy: 0.9744\n",
            "Epoch 16/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0621 - accuracy: 0.9766\n",
            "Epoch 17/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0663 - accuracy: 0.9750\n",
            "Epoch 18/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0506 - accuracy: 0.9800\n",
            "Epoch 19/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0521 - accuracy: 0.9820\n",
            "Epoch 20/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0471 - accuracy: 0.9829\n",
            "Epoch 21/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0465 - accuracy: 0.9831\n",
            "Epoch 22/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0426 - accuracy: 0.9846\n",
            "Epoch 23/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0378 - accuracy: 0.9859\n",
            "Epoch 24/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0369 - accuracy: 0.9870\n",
            "Epoch 25/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0313 - accuracy: 0.9892\n",
            "Epoch 26/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0394 - accuracy: 0.9859\n",
            "Epoch 27/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0410 - accuracy: 0.9862\n",
            "Epoch 28/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0352 - accuracy: 0.9875\n",
            "Epoch 29/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0335 - accuracy: 0.9878\n",
            "Epoch 30/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0338 - accuracy: 0.9882\n",
            "Epoch 31/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0332 - accuracy: 0.9881\n",
            "Epoch 32/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0332 - accuracy: 0.9878\n",
            "Epoch 33/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0286 - accuracy: 0.9906\n",
            "Epoch 34/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0249 - accuracy: 0.9912\n",
            "Epoch 35/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0223 - accuracy: 0.9926\n",
            "Epoch 36/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0309 - accuracy: 0.9899\n",
            "Epoch 37/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0299 - accuracy: 0.9896\n",
            "Epoch 38/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0289 - accuracy: 0.9905\n",
            "Epoch 39/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0257 - accuracy: 0.9923\n",
            "Epoch 40/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0229 - accuracy: 0.9921\n",
            "Epoch 41/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0191 - accuracy: 0.9930\n",
            "Epoch 42/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0222 - accuracy: 0.9916\n",
            "Epoch 43/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0222 - accuracy: 0.9920\n",
            "Epoch 44/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0176 - accuracy: 0.9941\n",
            "Epoch 45/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0220 - accuracy: 0.9923\n",
            "Epoch 46/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0277 - accuracy: 0.9911\n",
            "Epoch 47/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0287 - accuracy: 0.9909\n",
            "Epoch 48/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0275 - accuracy: 0.9911\n",
            "Epoch 49/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0237 - accuracy: 0.9925\n",
            "Epoch 50/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0191 - accuracy: 0.9944\n",
            "Epoch 51/100\n",
            "138/138 [==============================] - 2s 16ms/step - loss: 0.0234 - accuracy: 0.9929\n",
            "Epoch 52/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0227 - accuracy: 0.9925\n",
            "Epoch 53/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0219 - accuracy: 0.9923\n",
            "Epoch 54/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0255 - accuracy: 0.9917\n",
            "Epoch 55/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0235 - accuracy: 0.9928\n",
            "Epoch 56/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0165 - accuracy: 0.9945\n",
            "Epoch 57/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0166 - accuracy: 0.9945\n",
            "Epoch 58/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0136 - accuracy: 0.9953\n",
            "Epoch 59/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0190 - accuracy: 0.9937\n",
            "Epoch 60/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0216 - accuracy: 0.9931\n",
            "Epoch 61/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0199 - accuracy: 0.9936\n",
            "Epoch 62/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0162 - accuracy: 0.9951\n",
            "Epoch 63/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0185 - accuracy: 0.9935\n",
            "Epoch 64/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0181 - accuracy: 0.9937\n",
            "Epoch 65/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0181 - accuracy: 0.9944\n",
            "Epoch 66/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0122 - accuracy: 0.9957\n",
            "Epoch 67/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0179 - accuracy: 0.9934\n",
            "Epoch 68/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0190 - accuracy: 0.9935\n",
            "Epoch 69/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0154 - accuracy: 0.9952\n",
            "Epoch 70/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 71/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0177 - accuracy: 0.9941\n",
            "Epoch 72/100\n",
            "138/138 [==============================] - 2s 12ms/step - loss: 0.0199 - accuracy: 0.9937\n",
            "Epoch 73/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0188 - accuracy: 0.9946\n",
            "Epoch 74/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0149 - accuracy: 0.9945\n",
            "Epoch 75/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0221 - accuracy: 0.9935\n",
            "Epoch 76/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0180 - accuracy: 0.9952\n",
            "Epoch 77/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0140 - accuracy: 0.9961\n",
            "Epoch 78/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0162 - accuracy: 0.9946\n",
            "Epoch 79/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0168 - accuracy: 0.9951\n",
            "Epoch 80/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0171 - accuracy: 0.9944\n",
            "Epoch 81/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0145 - accuracy: 0.9956\n",
            "Epoch 82/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0149 - accuracy: 0.9948\n",
            "Epoch 83/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0176 - accuracy: 0.9950\n",
            "Epoch 84/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0174 - accuracy: 0.9948\n",
            "Epoch 85/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0203 - accuracy: 0.9945\n",
            "Epoch 86/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0198 - accuracy: 0.9949\n",
            "Epoch 87/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0116 - accuracy: 0.9966\n",
            "Epoch 88/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0149 - accuracy: 0.9953\n",
            "Epoch 89/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0154 - accuracy: 0.9950\n",
            "Epoch 90/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0158 - accuracy: 0.9956\n",
            "Epoch 91/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0136 - accuracy: 0.9958\n",
            "Epoch 92/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0152 - accuracy: 0.9954\n",
            "Epoch 93/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0114 - accuracy: 0.9961\n",
            "Epoch 94/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0165 - accuracy: 0.9952\n",
            "Epoch 95/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0119 - accuracy: 0.9962\n",
            "Epoch 96/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0112 - accuracy: 0.9963\n",
            "Epoch 97/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0084 - accuracy: 0.9972\n",
            "Epoch 98/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0123 - accuracy: 0.9970\n",
            "Epoch 99/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0118 - accuracy: 0.9968\n",
            "Epoch 100/100\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.0101 - accuracy: 0.9968\n",
            "69/69 [==============================] - 1s 5ms/step - loss: 0.8720 - accuracy: 0.7935\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.5041 - accuracy: 0.7095\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3920 - accuracy: 0.7911\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3488 - accuracy: 0.8165\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3102 - accuracy: 0.8456\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2711 - accuracy: 0.8683\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2421 - accuracy: 0.8835\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2051 - accuracy: 0.9062\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1659 - accuracy: 0.9274\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1403 - accuracy: 0.9403\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1156 - accuracy: 0.9542\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0969 - accuracy: 0.9626\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0795 - accuracy: 0.9692\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0676 - accuracy: 0.9751\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0591 - accuracy: 0.9778\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0528 - accuracy: 0.9814\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0417 - accuracy: 0.9858\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0349 - accuracy: 0.9879\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0366 - accuracy: 0.9864\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0391 - accuracy: 0.9851\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0369 - accuracy: 0.9862\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.5070 - accuracy: 0.8022\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.4948 - accuracy: 0.7142\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3853 - accuracy: 0.7947\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3442 - accuracy: 0.8188\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3076 - accuracy: 0.8423\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2749 - accuracy: 0.8652\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2368 - accuracy: 0.8885\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2010 - accuracy: 0.9096\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1665 - accuracy: 0.9267\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1467 - accuracy: 0.9362\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1200 - accuracy: 0.9514\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0944 - accuracy: 0.9622\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0710 - accuracy: 0.9740\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0600 - accuracy: 0.9790\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0641 - accuracy: 0.9765\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0568 - accuracy: 0.9790\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0481 - accuracy: 0.9824\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0373 - accuracy: 0.9864\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0358 - accuracy: 0.9874\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0315 - accuracy: 0.9886\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0368 - accuracy: 0.9872\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.5135 - accuracy: 0.8015\n",
            "Epoch 1/20\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.4955 - accuracy: 0.7149\n",
            "Epoch 2/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3902 - accuracy: 0.7934\n",
            "Epoch 3/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3517 - accuracy: 0.8150\n",
            "Epoch 4/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3078 - accuracy: 0.8432\n",
            "Epoch 5/20\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.2784 - accuracy: 0.8631\n",
            "Epoch 6/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2375 - accuracy: 0.8885\n",
            "Epoch 7/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2034 - accuracy: 0.9086\n",
            "Epoch 8/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1655 - accuracy: 0.9285\n",
            "Epoch 9/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1388 - accuracy: 0.9403\n",
            "Epoch 10/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1135 - accuracy: 0.9539\n",
            "Epoch 11/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0929 - accuracy: 0.9636\n",
            "Epoch 12/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0792 - accuracy: 0.9702\n",
            "Epoch 13/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0732 - accuracy: 0.9722\n",
            "Epoch 14/20\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0618 - accuracy: 0.9774\n",
            "Epoch 15/20\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0534 - accuracy: 0.9800\n",
            "Epoch 16/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0483 - accuracy: 0.9815\n",
            "Epoch 17/20\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0354 - accuracy: 0.9877\n",
            "Epoch 18/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0313 - accuracy: 0.9883\n",
            "Epoch 19/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0335 - accuracy: 0.9878\n",
            "Epoch 20/20\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0340 - accuracy: 0.9873\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5299 - accuracy: 0.7908\n",
            "Epoch 1/60\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.5017 - accuracy: 0.7095\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3861 - accuracy: 0.7919\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3444 - accuracy: 0.8203\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3080 - accuracy: 0.8421\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2743 - accuracy: 0.8663\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2418 - accuracy: 0.8858\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1949 - accuracy: 0.9132\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1698 - accuracy: 0.9250\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1351 - accuracy: 0.9429\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1126 - accuracy: 0.9533\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0912 - accuracy: 0.9641\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0746 - accuracy: 0.9714\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0621 - accuracy: 0.9781\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0602 - accuracy: 0.9777\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0504 - accuracy: 0.9825\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0420 - accuracy: 0.9842\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0433 - accuracy: 0.9850\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0383 - accuracy: 0.9860\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0374 - accuracy: 0.9864\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0354 - accuracy: 0.9884\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0381 - accuracy: 0.9857\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0333 - accuracy: 0.9878\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0283 - accuracy: 0.9906\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0315 - accuracy: 0.9890\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0288 - accuracy: 0.9896\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0227 - accuracy: 0.9918\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0243 - accuracy: 0.9906\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0246 - accuracy: 0.9912\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0238 - accuracy: 0.9914\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0196 - accuracy: 0.9920\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0238 - accuracy: 0.9920\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0229 - accuracy: 0.9923\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0165 - accuracy: 0.9942\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0176 - accuracy: 0.9936\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0173 - accuracy: 0.9946\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0225 - accuracy: 0.9931\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0204 - accuracy: 0.9930\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0187 - accuracy: 0.9932\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0282 - accuracy: 0.9905\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0239 - accuracy: 0.9918\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0193 - accuracy: 0.9930\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0202 - accuracy: 0.9935\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0240 - accuracy: 0.9926\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0211 - accuracy: 0.9931\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0223 - accuracy: 0.9929\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0172 - accuracy: 0.9937\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0142 - accuracy: 0.9949\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0118 - accuracy: 0.9956\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0179 - accuracy: 0.9939\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0207 - accuracy: 0.9933\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0222 - accuracy: 0.9926\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0156 - accuracy: 0.9945\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0147 - accuracy: 0.9949\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0129 - accuracy: 0.9960\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0185 - accuracy: 0.9942\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0136 - accuracy: 0.9951\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0163 - accuracy: 0.9946\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0158 - accuracy: 0.9950\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0151 - accuracy: 0.9945\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.7184 - accuracy: 0.7977\n",
            "Epoch 1/60\n",
            "46/46 [==============================] - 3s 18ms/step - loss: 0.5019 - accuracy: 0.7123\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3858 - accuracy: 0.7933\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3393 - accuracy: 0.8262\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3021 - accuracy: 0.8473\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2693 - accuracy: 0.8684\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2305 - accuracy: 0.8909\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1895 - accuracy: 0.9140\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1661 - accuracy: 0.9269\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1286 - accuracy: 0.9487\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1084 - accuracy: 0.9558\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0913 - accuracy: 0.9634\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0710 - accuracy: 0.9731\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0618 - accuracy: 0.9773\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0536 - accuracy: 0.9792\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0531 - accuracy: 0.9809\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0477 - accuracy: 0.9829\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0371 - accuracy: 0.9867\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0340 - accuracy: 0.9872\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0378 - accuracy: 0.9869\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0344 - accuracy: 0.9873\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0301 - accuracy: 0.9895\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0277 - accuracy: 0.9897\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0284 - accuracy: 0.9904\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0271 - accuracy: 0.9902\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0255 - accuracy: 0.9909\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0232 - accuracy: 0.9922\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0236 - accuracy: 0.9916\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0238 - accuracy: 0.9915\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0207 - accuracy: 0.9928\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0180 - accuracy: 0.9938\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0202 - accuracy: 0.9931\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0237 - accuracy: 0.9909\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0249 - accuracy: 0.9910\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0245 - accuracy: 0.9914\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0252 - accuracy: 0.9912\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0279 - accuracy: 0.9904\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0225 - accuracy: 0.9923\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0203 - accuracy: 0.9928\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0201 - accuracy: 0.9925\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0162 - accuracy: 0.9942\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0199 - accuracy: 0.9928\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0208 - accuracy: 0.9931\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0164 - accuracy: 0.9943\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0222 - accuracy: 0.9922\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0159 - accuracy: 0.9945\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0212 - accuracy: 0.9933\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0182 - accuracy: 0.9936\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0182 - accuracy: 0.9941\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0196 - accuracy: 0.9934\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0175 - accuracy: 0.9940\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0173 - accuracy: 0.9938\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0166 - accuracy: 0.9939\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0163 - accuracy: 0.9944\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0175 - accuracy: 0.9940\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0143 - accuracy: 0.9946\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9942\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0138 - accuracy: 0.9950\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9947\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0181 - accuracy: 0.9943\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0145 - accuracy: 0.9952\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.7684 - accuracy: 0.7872\n",
            "Epoch 1/60\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.4976 - accuracy: 0.7155\n",
            "Epoch 2/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3881 - accuracy: 0.7916\n",
            "Epoch 3/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3455 - accuracy: 0.8239\n",
            "Epoch 4/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3093 - accuracy: 0.8447\n",
            "Epoch 5/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2709 - accuracy: 0.8691\n",
            "Epoch 6/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2387 - accuracy: 0.8857\n",
            "Epoch 7/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1994 - accuracy: 0.9119\n",
            "Epoch 8/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1655 - accuracy: 0.9273\n",
            "Epoch 9/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1353 - accuracy: 0.9411\n",
            "Epoch 10/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1099 - accuracy: 0.9543\n",
            "Epoch 11/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0952 - accuracy: 0.9607\n",
            "Epoch 12/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0821 - accuracy: 0.9672\n",
            "Epoch 13/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0666 - accuracy: 0.9764\n",
            "Epoch 14/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0561 - accuracy: 0.9794\n",
            "Epoch 15/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0493 - accuracy: 0.9814\n",
            "Epoch 16/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0403 - accuracy: 0.9849\n",
            "Epoch 17/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0385 - accuracy: 0.9861\n",
            "Epoch 18/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0375 - accuracy: 0.9859\n",
            "Epoch 19/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0332 - accuracy: 0.9896\n",
            "Epoch 20/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0322 - accuracy: 0.9885\n",
            "Epoch 21/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0323 - accuracy: 0.9878\n",
            "Epoch 22/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0304 - accuracy: 0.9884\n",
            "Epoch 23/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0307 - accuracy: 0.9888\n",
            "Epoch 24/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0293 - accuracy: 0.9888\n",
            "Epoch 25/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0292 - accuracy: 0.9891\n",
            "Epoch 26/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0238 - accuracy: 0.9917\n",
            "Epoch 27/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0271 - accuracy: 0.9903\n",
            "Epoch 28/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0258 - accuracy: 0.9903\n",
            "Epoch 29/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0265 - accuracy: 0.9906\n",
            "Epoch 30/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0238 - accuracy: 0.9910\n",
            "Epoch 31/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0253 - accuracy: 0.9909\n",
            "Epoch 32/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0248 - accuracy: 0.9908\n",
            "Epoch 33/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0269 - accuracy: 0.9912\n",
            "Epoch 34/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0240 - accuracy: 0.9904\n",
            "Epoch 35/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0216 - accuracy: 0.9923\n",
            "Epoch 36/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0265 - accuracy: 0.9909\n",
            "Epoch 37/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0198 - accuracy: 0.9931\n",
            "Epoch 38/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0179 - accuracy: 0.9939\n",
            "Epoch 39/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0179 - accuracy: 0.9941\n",
            "Epoch 40/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0193 - accuracy: 0.9929\n",
            "Epoch 41/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0198 - accuracy: 0.9931\n",
            "Epoch 42/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0195 - accuracy: 0.9934\n",
            "Epoch 43/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0217 - accuracy: 0.9927\n",
            "Epoch 44/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0211 - accuracy: 0.9930\n",
            "Epoch 45/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0168 - accuracy: 0.9942\n",
            "Epoch 46/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0179 - accuracy: 0.9938\n",
            "Epoch 47/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0158 - accuracy: 0.9937\n",
            "Epoch 48/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0199 - accuracy: 0.9932\n",
            "Epoch 49/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0194 - accuracy: 0.9936\n",
            "Epoch 50/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0181 - accuracy: 0.9935\n",
            "Epoch 51/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0176 - accuracy: 0.9936\n",
            "Epoch 52/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 53/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0145 - accuracy: 0.9953\n",
            "Epoch 54/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0109 - accuracy: 0.9963\n",
            "Epoch 55/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0116 - accuracy: 0.9959\n",
            "Epoch 56/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0136 - accuracy: 0.9952\n",
            "Epoch 57/60\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0135 - accuracy: 0.9951\n",
            "Epoch 58/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0135 - accuracy: 0.9955\n",
            "Epoch 59/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0154 - accuracy: 0.9951\n",
            "Epoch 60/60\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0148 - accuracy: 0.9955\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7595 - accuracy: 0.7910\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.4969 - accuracy: 0.7122\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3871 - accuracy: 0.7929\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3458 - accuracy: 0.8206\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3095 - accuracy: 0.8441\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2746 - accuracy: 0.8645\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2371 - accuracy: 0.8875\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1987 - accuracy: 0.9088\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1701 - accuracy: 0.9235\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1346 - accuracy: 0.9451\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1064 - accuracy: 0.9568\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0944 - accuracy: 0.9634\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0818 - accuracy: 0.9687\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0654 - accuracy: 0.9744\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0609 - accuracy: 0.9778\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0499 - accuracy: 0.9819\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0389 - accuracy: 0.9861\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0376 - accuracy: 0.9872\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0390 - accuracy: 0.9853\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0348 - accuracy: 0.9872\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0366 - accuracy: 0.9872\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0376 - accuracy: 0.9871\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0315 - accuracy: 0.9886\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0293 - accuracy: 0.9894\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0238 - accuracy: 0.9910\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0235 - accuracy: 0.9921\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0246 - accuracy: 0.9909\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0226 - accuracy: 0.9928\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0257 - accuracy: 0.9912\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0231 - accuracy: 0.9919\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0219 - accuracy: 0.9925\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0263 - accuracy: 0.9910\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0269 - accuracy: 0.9900\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0240 - accuracy: 0.9913\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0230 - accuracy: 0.9918\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0211 - accuracy: 0.9922\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0223 - accuracy: 0.9917\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0279 - accuracy: 0.9898\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0286 - accuracy: 0.9912\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0190 - accuracy: 0.9935\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0191 - accuracy: 0.9935\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0167 - accuracy: 0.9941\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0196 - accuracy: 0.9922\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0189 - accuracy: 0.9935\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0156 - accuracy: 0.9944\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0159 - accuracy: 0.9942\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0192 - accuracy: 0.9928\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0189 - accuracy: 0.9931\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0152 - accuracy: 0.9944\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9949\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0154 - accuracy: 0.9948\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0178 - accuracy: 0.9936\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0180 - accuracy: 0.9935\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0167 - accuracy: 0.9944\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0133 - accuracy: 0.9953\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0190 - accuracy: 0.9930\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0169 - accuracy: 0.9938\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0183 - accuracy: 0.9933\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0146 - accuracy: 0.9951\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0139 - accuracy: 0.9951\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0113 - accuracy: 0.9957\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0097 - accuracy: 0.9964\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0119 - accuracy: 0.9959\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0131 - accuracy: 0.9954\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0135 - accuracy: 0.9952\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0145 - accuracy: 0.9952\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0144 - accuracy: 0.9946\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0150 - accuracy: 0.9955\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0141 - accuracy: 0.9952\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0139 - accuracy: 0.9958\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0188 - accuracy: 0.9939\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0184 - accuracy: 0.9938\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0138 - accuracy: 0.9959\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0171 - accuracy: 0.9946\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0205 - accuracy: 0.9940\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0195 - accuracy: 0.9939\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0129 - accuracy: 0.9960\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0141 - accuracy: 0.9954\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0115 - accuracy: 0.9964\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0150 - accuracy: 0.9956\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0116 - accuracy: 0.9963\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0102 - accuracy: 0.9967\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0126 - accuracy: 0.9961\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0187 - accuracy: 0.9948\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0152 - accuracy: 0.9952\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0176 - accuracy: 0.9943\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0144 - accuracy: 0.9953\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0125 - accuracy: 0.9963\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9949\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0126 - accuracy: 0.9961\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0114 - accuracy: 0.9966\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9964\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0097 - accuracy: 0.9970\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0109 - accuracy: 0.9957\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0139 - accuracy: 0.9958\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0150 - accuracy: 0.9953\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0154 - accuracy: 0.9952\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0113 - accuracy: 0.9959\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0120 - accuracy: 0.9961\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0112 - accuracy: 0.9962\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0108 - accuracy: 0.9960\n",
            "23/23 [==============================] - 1s 7ms/step - loss: 0.8596 - accuracy: 0.7974\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 4s 19ms/step - loss: 0.4920 - accuracy: 0.7204\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3838 - accuracy: 0.7913\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3405 - accuracy: 0.8239\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3098 - accuracy: 0.8430\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2746 - accuracy: 0.8661\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2346 - accuracy: 0.8875\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1943 - accuracy: 0.9099\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.1558 - accuracy: 0.9322\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1376 - accuracy: 0.9424\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1137 - accuracy: 0.9528\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0981 - accuracy: 0.9616\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0804 - accuracy: 0.9689\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0683 - accuracy: 0.9733\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0549 - accuracy: 0.9786\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0501 - accuracy: 0.9814\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0464 - accuracy: 0.9830\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0387 - accuracy: 0.9864\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0366 - accuracy: 0.9876\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0307 - accuracy: 0.9889\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0297 - accuracy: 0.9896\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0290 - accuracy: 0.9899\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0280 - accuracy: 0.9902\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0237 - accuracy: 0.9916\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0237 - accuracy: 0.9915\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0267 - accuracy: 0.9910\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0245 - accuracy: 0.9917\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0294 - accuracy: 0.9900\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0359 - accuracy: 0.9869\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0326 - accuracy: 0.9869\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0295 - accuracy: 0.9898\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0254 - accuracy: 0.9911\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0251 - accuracy: 0.9912\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0240 - accuracy: 0.9907\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0234 - accuracy: 0.9922\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0236 - accuracy: 0.9920\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0167 - accuracy: 0.9936\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0182 - accuracy: 0.9934\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0186 - accuracy: 0.9940\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0168 - accuracy: 0.9944\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0158 - accuracy: 0.9945\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0158 - accuracy: 0.9943\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0169 - accuracy: 0.9942\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0198 - accuracy: 0.9935\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0222 - accuracy: 0.9931\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0160 - accuracy: 0.9940\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0160 - accuracy: 0.9944\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0190 - accuracy: 0.9931\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0195 - accuracy: 0.9931\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0183 - accuracy: 0.9930\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0180 - accuracy: 0.9943\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0206 - accuracy: 0.9929\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0188 - accuracy: 0.9936\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9955\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0179 - accuracy: 0.9940\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0131 - accuracy: 0.9957\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9941\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0131 - accuracy: 0.9954\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0152 - accuracy: 0.9947\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0149 - accuracy: 0.9946\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0147 - accuracy: 0.9946\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0168 - accuracy: 0.9943\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0205 - accuracy: 0.9929\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0159 - accuracy: 0.9944\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0127 - accuracy: 0.9955\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0164 - accuracy: 0.9941\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0144 - accuracy: 0.9943\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0219 - accuracy: 0.9938\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0168 - accuracy: 0.9943\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0159 - accuracy: 0.9940\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0203 - accuracy: 0.9940\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0193 - accuracy: 0.9944\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0145 - accuracy: 0.9952\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0109 - accuracy: 0.9963\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0118 - accuracy: 0.9958\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0106 - accuracy: 0.9965\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0102 - accuracy: 0.9963\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0116 - accuracy: 0.9966\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0102 - accuracy: 0.9967\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0137 - accuracy: 0.9952\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0134 - accuracy: 0.9960\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0107 - accuracy: 0.9962\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0118 - accuracy: 0.9963\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0106 - accuracy: 0.9963\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0116 - accuracy: 0.9962\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0132 - accuracy: 0.9961\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0136 - accuracy: 0.9963\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0118 - accuracy: 0.9961\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0113 - accuracy: 0.9964\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0112 - accuracy: 0.9963\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0133 - accuracy: 0.9960\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0175 - accuracy: 0.9946\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0173 - accuracy: 0.9943\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0169 - accuracy: 0.9941\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0150 - accuracy: 0.9955\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0193 - accuracy: 0.9940\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0115 - accuracy: 0.9960\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8102 - accuracy: 0.7962\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.4970 - accuracy: 0.7153\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3881 - accuracy: 0.7943\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3484 - accuracy: 0.8206\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.3115 - accuracy: 0.8420\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2770 - accuracy: 0.8643\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2352 - accuracy: 0.8889\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.2022 - accuracy: 0.9074\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1686 - accuracy: 0.9255\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1472 - accuracy: 0.9386\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.1161 - accuracy: 0.9528\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0950 - accuracy: 0.9628\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0818 - accuracy: 0.9675\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0695 - accuracy: 0.9733\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0577 - accuracy: 0.9786\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0530 - accuracy: 0.9808\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0510 - accuracy: 0.9814\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0393 - accuracy: 0.9858\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0409 - accuracy: 0.9852\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0326 - accuracy: 0.9882\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0335 - accuracy: 0.9873\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0342 - accuracy: 0.9868\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0341 - accuracy: 0.9876\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0255 - accuracy: 0.9909\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0253 - accuracy: 0.9905\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0281 - accuracy: 0.9905\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0294 - accuracy: 0.9902\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 0.0310 - accuracy: 0.9887\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0274 - accuracy: 0.9896\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0255 - accuracy: 0.9919\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0232 - accuracy: 0.9916\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0230 - accuracy: 0.9923\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0225 - accuracy: 0.9923\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0238 - accuracy: 0.9914\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0237 - accuracy: 0.9918\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0252 - accuracy: 0.9909\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0211 - accuracy: 0.9930\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0233 - accuracy: 0.9930\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0199 - accuracy: 0.9929\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0234 - accuracy: 0.9922\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0176 - accuracy: 0.9939\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0180 - accuracy: 0.9940\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0229 - accuracy: 0.9924\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0195 - accuracy: 0.9935\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0205 - accuracy: 0.9931\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0173 - accuracy: 0.9938\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0184 - accuracy: 0.9930\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0189 - accuracy: 0.9932\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0164 - accuracy: 0.9940\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0166 - accuracy: 0.9943\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0168 - accuracy: 0.9940\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0163 - accuracy: 0.9950\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0153 - accuracy: 0.9944\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0198 - accuracy: 0.9938\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0148 - accuracy: 0.9956\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0132 - accuracy: 0.9954\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0114 - accuracy: 0.9958\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0178 - accuracy: 0.9946\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0115 - accuracy: 0.9966\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0157 - accuracy: 0.9946\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0136 - accuracy: 0.9955\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0217 - accuracy: 0.9927\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0170 - accuracy: 0.9948\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0210 - accuracy: 0.9935\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0176 - accuracy: 0.9938\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0199 - accuracy: 0.9939\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0133 - accuracy: 0.9954\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0133 - accuracy: 0.9958\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0138 - accuracy: 0.9956\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0136 - accuracy: 0.9962\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0147 - accuracy: 0.9955\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0139 - accuracy: 0.9952\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0179 - accuracy: 0.9941\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0165 - accuracy: 0.9949\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0163 - accuracy: 0.9945\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0155 - accuracy: 0.9956\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0137 - accuracy: 0.9950\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0112 - accuracy: 0.9959\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0150 - accuracy: 0.9948\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0137 - accuracy: 0.9953\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0127 - accuracy: 0.9959\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0146 - accuracy: 0.9955\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0139 - accuracy: 0.9954\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0155 - accuracy: 0.9955\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0170 - accuracy: 0.9946\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0123 - accuracy: 0.9963\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0121 - accuracy: 0.9965\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0121 - accuracy: 0.9962\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0131 - accuracy: 0.9958\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0139 - accuracy: 0.9954\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0143 - accuracy: 0.9951\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0158 - accuracy: 0.9944\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0117 - accuracy: 0.9961\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9959\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0113 - accuracy: 0.9963\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9956\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0113 - accuracy: 0.9958\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 0.9960\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 1s 18ms/step - loss: 0.0112 - accuracy: 0.9963\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 0.0084 - accuracy: 0.9974\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8326 - accuracy: 0.7957\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5162 - accuracy: 0.6997\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.3913 - accuracy: 0.7935\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3502 - accuracy: 0.8157\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3115 - accuracy: 0.8423\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2717 - accuracy: 0.8677\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2368 - accuracy: 0.8888\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2081 - accuracy: 0.9045\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1725 - accuracy: 0.9266\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1352 - accuracy: 0.9444\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1179 - accuracy: 0.9506\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0969 - accuracy: 0.9619\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0792 - accuracy: 0.9691\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0602 - accuracy: 0.9787\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0558 - accuracy: 0.9801\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0492 - accuracy: 0.9824\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0442 - accuracy: 0.9839\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0389 - accuracy: 0.9869\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0340 - accuracy: 0.9880\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0355 - accuracy: 0.9871\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0356 - accuracy: 0.9878\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4898 - accuracy: 0.7996\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5215 - accuracy: 0.6993\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3858 - accuracy: 0.7946\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3435 - accuracy: 0.8234\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3052 - accuracy: 0.8465\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2653 - accuracy: 0.8710\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2265 - accuracy: 0.8936\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1856 - accuracy: 0.9193\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1541 - accuracy: 0.9367\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1222 - accuracy: 0.9507\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1040 - accuracy: 0.9601\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0838 - accuracy: 0.9696\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0635 - accuracy: 0.9761\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0549 - accuracy: 0.9810\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0521 - accuracy: 0.9818\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0453 - accuracy: 0.9840\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0369 - accuracy: 0.9873\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0384 - accuracy: 0.9862\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0335 - accuracy: 0.9878\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0259 - accuracy: 0.9906\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0284 - accuracy: 0.9900\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.5105 - accuracy: 0.7994\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5162 - accuracy: 0.7002\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3927 - accuracy: 0.7915\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3500 - accuracy: 0.8184\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3105 - accuracy: 0.8473\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2772 - accuracy: 0.8644\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2382 - accuracy: 0.8904\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1980 - accuracy: 0.9117\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1751 - accuracy: 0.9226\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.1409 - accuracy: 0.9410\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1072 - accuracy: 0.9557\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0915 - accuracy: 0.9647\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0731 - accuracy: 0.9722\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0629 - accuracy: 0.9762\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0564 - accuracy: 0.9793\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0553 - accuracy: 0.9797\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0479 - accuracy: 0.9829\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0357 - accuracy: 0.9879\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0339 - accuracy: 0.9879\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0339 - accuracy: 0.9889\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0318 - accuracy: 0.9898\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4980 - accuracy: 0.7941\n",
            "Epoch 1/60\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5166 - accuracy: 0.7013\n",
            "Epoch 2/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3894 - accuracy: 0.7895\n",
            "Epoch 3/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3477 - accuracy: 0.8192\n",
            "Epoch 4/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3130 - accuracy: 0.8425\n",
            "Epoch 5/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2694 - accuracy: 0.8686\n",
            "Epoch 6/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.2309 - accuracy: 0.8926\n",
            "Epoch 7/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1999 - accuracy: 0.9106\n",
            "Epoch 8/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1696 - accuracy: 0.9254\n",
            "Epoch 9/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1364 - accuracy: 0.9419\n",
            "Epoch 10/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1197 - accuracy: 0.9516\n",
            "Epoch 11/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0938 - accuracy: 0.9620\n",
            "Epoch 12/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0811 - accuracy: 0.9688\n",
            "Epoch 13/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0665 - accuracy: 0.9746\n",
            "Epoch 14/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0524 - accuracy: 0.9812\n",
            "Epoch 15/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0419 - accuracy: 0.9858\n",
            "Epoch 16/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0368 - accuracy: 0.9866\n",
            "Epoch 17/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0336 - accuracy: 0.9877\n",
            "Epoch 18/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0323 - accuracy: 0.9887\n",
            "Epoch 19/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0306 - accuracy: 0.9899\n",
            "Epoch 20/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0308 - accuracy: 0.9886\n",
            "Epoch 21/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0288 - accuracy: 0.9900\n",
            "Epoch 22/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0280 - accuracy: 0.9903\n",
            "Epoch 23/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0281 - accuracy: 0.9902\n",
            "Epoch 24/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0308 - accuracy: 0.9887\n",
            "Epoch 25/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0274 - accuracy: 0.9905\n",
            "Epoch 26/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0282 - accuracy: 0.9897\n",
            "Epoch 27/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0249 - accuracy: 0.9916\n",
            "Epoch 28/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9938\n",
            "Epoch 29/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0182 - accuracy: 0.9935\n",
            "Epoch 30/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0178 - accuracy: 0.9943\n",
            "Epoch 31/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0164 - accuracy: 0.9942\n",
            "Epoch 32/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9938\n",
            "Epoch 33/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0215 - accuracy: 0.9926\n",
            "Epoch 34/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 35/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0208 - accuracy: 0.9933\n",
            "Epoch 36/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0211 - accuracy: 0.9931\n",
            "Epoch 37/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0220 - accuracy: 0.9924\n",
            "Epoch 38/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0217 - accuracy: 0.9923\n",
            "Epoch 39/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0229 - accuracy: 0.9924\n",
            "Epoch 40/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9939\n",
            "Epoch 41/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0163 - accuracy: 0.9948\n",
            "Epoch 42/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9948\n",
            "Epoch 43/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9948\n",
            "Epoch 44/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9956\n",
            "Epoch 45/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0143 - accuracy: 0.9946\n",
            "Epoch 46/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0158 - accuracy: 0.9946\n",
            "Epoch 47/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0136 - accuracy: 0.9946\n",
            "Epoch 48/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0153 - accuracy: 0.9944\n",
            "Epoch 49/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0132 - accuracy: 0.9958\n",
            "Epoch 50/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0132 - accuracy: 0.9950\n",
            "Epoch 51/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9955\n",
            "Epoch 52/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9958\n",
            "Epoch 53/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0116 - accuracy: 0.9960\n",
            "Epoch 54/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9957\n",
            "Epoch 55/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9956\n",
            "Epoch 56/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0146 - accuracy: 0.9950\n",
            "Epoch 57/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0097 - accuracy: 0.9965\n",
            "Epoch 58/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0114 - accuracy: 0.9959\n",
            "Epoch 59/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0125 - accuracy: 0.9959\n",
            "Epoch 60/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9947\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6390 - accuracy: 0.8031\n",
            "Epoch 1/60\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5159 - accuracy: 0.6995\n",
            "Epoch 2/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3830 - accuracy: 0.7954\n",
            "Epoch 3/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3388 - accuracy: 0.8249\n",
            "Epoch 4/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2970 - accuracy: 0.8516\n",
            "Epoch 5/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2664 - accuracy: 0.8710\n",
            "Epoch 6/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2294 - accuracy: 0.8945\n",
            "Epoch 7/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1932 - accuracy: 0.9122\n",
            "Epoch 8/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1622 - accuracy: 0.9290\n",
            "Epoch 9/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1273 - accuracy: 0.9477\n",
            "Epoch 10/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1104 - accuracy: 0.9562\n",
            "Epoch 11/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0894 - accuracy: 0.9645\n",
            "Epoch 12/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0768 - accuracy: 0.9699\n",
            "Epoch 13/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0612 - accuracy: 0.9782\n",
            "Epoch 14/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0464 - accuracy: 0.9843\n",
            "Epoch 15/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0439 - accuracy: 0.9842\n",
            "Epoch 16/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0372 - accuracy: 0.9871\n",
            "Epoch 17/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0346 - accuracy: 0.9873\n",
            "Epoch 18/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0290 - accuracy: 0.9900\n",
            "Epoch 19/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0298 - accuracy: 0.9891\n",
            "Epoch 20/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0276 - accuracy: 0.9904\n",
            "Epoch 21/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0289 - accuracy: 0.9905\n",
            "Epoch 22/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0246 - accuracy: 0.9916\n",
            "Epoch 23/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0212 - accuracy: 0.9925\n",
            "Epoch 24/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0180 - accuracy: 0.9939\n",
            "Epoch 25/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0218 - accuracy: 0.9928\n",
            "Epoch 26/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0251 - accuracy: 0.9912\n",
            "Epoch 27/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0230 - accuracy: 0.9916\n",
            "Epoch 28/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0199 - accuracy: 0.9931\n",
            "Epoch 29/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0194 - accuracy: 0.9929\n",
            "Epoch 30/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0183 - accuracy: 0.9942\n",
            "Epoch 31/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0184 - accuracy: 0.9933\n",
            "Epoch 32/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0197 - accuracy: 0.9926\n",
            "Epoch 33/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0204 - accuracy: 0.9923\n",
            "Epoch 34/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 35/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0226 - accuracy: 0.9925\n",
            "Epoch 36/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0219 - accuracy: 0.9920\n",
            "Epoch 37/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9931\n",
            "Epoch 38/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9941\n",
            "Epoch 39/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0228 - accuracy: 0.9919\n",
            "Epoch 40/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0229 - accuracy: 0.9923\n",
            "Epoch 41/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0196 - accuracy: 0.9930\n",
            "Epoch 42/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9937\n",
            "Epoch 43/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0197 - accuracy: 0.9935\n",
            "Epoch 44/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0157 - accuracy: 0.9949\n",
            "Epoch 45/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9942\n",
            "Epoch 46/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9957\n",
            "Epoch 47/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0110 - accuracy: 0.9960\n",
            "Epoch 48/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9953\n",
            "Epoch 49/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9957\n",
            "Epoch 50/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9953\n",
            "Epoch 51/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9957\n",
            "Epoch 52/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0120 - accuracy: 0.9963\n",
            "Epoch 53/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0121 - accuracy: 0.9962\n",
            "Epoch 54/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9958\n",
            "Epoch 55/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 56/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0152 - accuracy: 0.9954\n",
            "Epoch 57/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9945\n",
            "Epoch 58/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0158 - accuracy: 0.9945\n",
            "Epoch 59/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0121 - accuracy: 0.9956\n",
            "Epoch 60/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0117 - accuracy: 0.9957\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6960 - accuracy: 0.7922\n",
            "Epoch 1/60\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5147 - accuracy: 0.6991\n",
            "Epoch 2/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3946 - accuracy: 0.7875\n",
            "Epoch 3/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3481 - accuracy: 0.8188\n",
            "Epoch 4/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3069 - accuracy: 0.8470\n",
            "Epoch 5/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2772 - accuracy: 0.8664\n",
            "Epoch 6/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2342 - accuracy: 0.8922\n",
            "Epoch 7/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1964 - accuracy: 0.9122\n",
            "Epoch 8/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.1605 - accuracy: 0.9309\n",
            "Epoch 9/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1457 - accuracy: 0.9385\n",
            "Epoch 10/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.1191 - accuracy: 0.9536\n",
            "Epoch 11/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0959 - accuracy: 0.9628\n",
            "Epoch 12/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0800 - accuracy: 0.9692\n",
            "Epoch 13/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0584 - accuracy: 0.9791\n",
            "Epoch 14/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0525 - accuracy: 0.9804\n",
            "Epoch 15/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0474 - accuracy: 0.9823\n",
            "Epoch 16/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0351 - accuracy: 0.9878\n",
            "Epoch 17/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0315 - accuracy: 0.9897\n",
            "Epoch 18/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0286 - accuracy: 0.9901\n",
            "Epoch 19/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0296 - accuracy: 0.9903\n",
            "Epoch 20/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0357 - accuracy: 0.9868\n",
            "Epoch 21/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0290 - accuracy: 0.9898\n",
            "Epoch 22/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0267 - accuracy: 0.9903\n",
            "Epoch 23/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0258 - accuracy: 0.9906\n",
            "Epoch 24/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0252 - accuracy: 0.9913\n",
            "Epoch 25/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0236 - accuracy: 0.9914\n",
            "Epoch 26/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0246 - accuracy: 0.9918\n",
            "Epoch 27/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0216 - accuracy: 0.9925\n",
            "Epoch 28/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0202 - accuracy: 0.9924\n",
            "Epoch 29/60\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0203 - accuracy: 0.9930\n",
            "Epoch 30/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9937\n",
            "Epoch 31/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0195 - accuracy: 0.9931\n",
            "Epoch 32/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0192 - accuracy: 0.9933\n",
            "Epoch 33/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0215 - accuracy: 0.9921\n",
            "Epoch 34/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9940\n",
            "Epoch 35/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0194 - accuracy: 0.9931\n",
            "Epoch 36/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0204 - accuracy: 0.9936\n",
            "Epoch 37/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0183 - accuracy: 0.9939\n",
            "Epoch 38/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0174 - accuracy: 0.9935\n",
            "Epoch 39/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9947\n",
            "Epoch 40/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0145 - accuracy: 0.9943\n",
            "Epoch 41/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0161 - accuracy: 0.9943\n",
            "Epoch 42/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0175 - accuracy: 0.9937\n",
            "Epoch 43/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0189 - accuracy: 0.9930\n",
            "Epoch 44/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0193 - accuracy: 0.9935\n",
            "Epoch 45/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0225 - accuracy: 0.9926\n",
            "Epoch 46/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0182 - accuracy: 0.9935\n",
            "Epoch 47/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0207 - accuracy: 0.9936\n",
            "Epoch 48/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9943\n",
            "Epoch 49/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0151 - accuracy: 0.9950\n",
            "Epoch 50/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0142 - accuracy: 0.9949\n",
            "Epoch 51/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 52/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0110 - accuracy: 0.9961\n",
            "Epoch 53/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9954\n",
            "Epoch 54/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9953\n",
            "Epoch 55/60\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0131 - accuracy: 0.9957\n",
            "Epoch 56/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9944\n",
            "Epoch 57/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9945\n",
            "Epoch 58/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0149 - accuracy: 0.9951\n",
            "Epoch 59/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9960\n",
            "Epoch 60/60\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0168 - accuracy: 0.9946\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.6509 - accuracy: 0.8002\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5261 - accuracy: 0.6884\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3950 - accuracy: 0.7910\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3462 - accuracy: 0.8179\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3102 - accuracy: 0.8453\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2687 - accuracy: 0.8682\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.2275 - accuracy: 0.8929\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1874 - accuracy: 0.9186\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1708 - accuracy: 0.9240\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1329 - accuracy: 0.9454\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1098 - accuracy: 0.9549\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0858 - accuracy: 0.9662\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0720 - accuracy: 0.9725\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0637 - accuracy: 0.9769\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0526 - accuracy: 0.9819\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0434 - accuracy: 0.9853\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0394 - accuracy: 0.9862\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0379 - accuracy: 0.9861\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0379 - accuracy: 0.9862\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0377 - accuracy: 0.9875\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0333 - accuracy: 0.9883\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0299 - accuracy: 0.9890\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0300 - accuracy: 0.9902\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0245 - accuracy: 0.9913\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0256 - accuracy: 0.9904\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0183 - accuracy: 0.9940\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0185 - accuracy: 0.9936\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0198 - accuracy: 0.9931\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0196 - accuracy: 0.9928\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0205 - accuracy: 0.9927\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0189 - accuracy: 0.9930\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0161 - accuracy: 0.9946\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0181 - accuracy: 0.9939\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0198 - accuracy: 0.9931\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0194 - accuracy: 0.9930\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0187 - accuracy: 0.9931\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9942\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0155 - accuracy: 0.9940\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0159 - accuracy: 0.9945\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0198 - accuracy: 0.9932\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0220 - accuracy: 0.9932\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0218 - accuracy: 0.9924\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0205 - accuracy: 0.9932\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0188 - accuracy: 0.9936\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9949\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9944\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0141 - accuracy: 0.9952\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9956\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0191 - accuracy: 0.9937\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9941\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0150 - accuracy: 0.9948\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0138 - accuracy: 0.9953\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0169 - accuracy: 0.9943\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9956\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0123 - accuracy: 0.9954\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0166 - accuracy: 0.9955\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0162 - accuracy: 0.9940\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0145 - accuracy: 0.9954\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0148 - accuracy: 0.9949\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9961\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9952\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0136 - accuracy: 0.9954\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0155 - accuracy: 0.9948\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9956\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0107 - accuracy: 0.9963\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0100 - accuracy: 0.9970\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9962\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9958\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0101 - accuracy: 0.9967\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0132 - accuracy: 0.9953\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9952\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9959\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0105 - accuracy: 0.9964\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0098 - accuracy: 0.9963\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0085 - accuracy: 0.9967\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0094 - accuracy: 0.9967\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0129 - accuracy: 0.9957\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9955\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0107 - accuracy: 0.9967\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0098 - accuracy: 0.9965\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0092 - accuracy: 0.9964\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0103 - accuracy: 0.9963\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0096 - accuracy: 0.9965\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0090 - accuracy: 0.9967\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0103 - accuracy: 0.9965\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0116 - accuracy: 0.9959\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0158 - accuracy: 0.9954\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0154 - accuracy: 0.9956\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0142 - accuracy: 0.9951\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9962\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9959\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0109 - accuracy: 0.9966\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9959\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0101 - accuracy: 0.9963\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0145 - accuracy: 0.9956\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9953\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0143 - accuracy: 0.9955\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0131 - accuracy: 0.9956\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0122 - accuracy: 0.9962\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0106 - accuracy: 0.9968\n",
            "14/14 [==============================] - 1s 9ms/step - loss: 0.7539 - accuracy: 0.7972\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5159 - accuracy: 0.7043\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3832 - accuracy: 0.7977\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3396 - accuracy: 0.8265\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3018 - accuracy: 0.8503\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2674 - accuracy: 0.8686\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2356 - accuracy: 0.8888\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1925 - accuracy: 0.9145\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1577 - accuracy: 0.9321\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1338 - accuracy: 0.9461\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1241 - accuracy: 0.9498\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0976 - accuracy: 0.9616\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0842 - accuracy: 0.9683\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0658 - accuracy: 0.9752\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0540 - accuracy: 0.9811\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0529 - accuracy: 0.9802\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0401 - accuracy: 0.9863\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0336 - accuracy: 0.9880\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0308 - accuracy: 0.9889\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0312 - accuracy: 0.9893\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0332 - accuracy: 0.9884\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0298 - accuracy: 0.9900\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0271 - accuracy: 0.9904\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0256 - accuracy: 0.9914\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0255 - accuracy: 0.9909\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0269 - accuracy: 0.9902\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0256 - accuracy: 0.9899\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0207 - accuracy: 0.9931\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0187 - accuracy: 0.9933\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0181 - accuracy: 0.9942\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0148 - accuracy: 0.9947\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0175 - accuracy: 0.9946\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0185 - accuracy: 0.9937\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0195 - accuracy: 0.9935\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0176 - accuracy: 0.9935\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0202 - accuracy: 0.9924\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9936\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0142 - accuracy: 0.9946\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0169 - accuracy: 0.9935\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0166 - accuracy: 0.9936\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0171 - accuracy: 0.9944\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0173 - accuracy: 0.9936\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9943\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9948\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9944\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0170 - accuracy: 0.9947\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0198 - accuracy: 0.9937\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0187 - accuracy: 0.9933\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0210 - accuracy: 0.9929\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0207 - accuracy: 0.9926\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0190 - accuracy: 0.9932\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0155 - accuracy: 0.9949\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0158 - accuracy: 0.9947\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9956\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0114 - accuracy: 0.9962\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0088 - accuracy: 0.9968\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0088 - accuracy: 0.9966\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0083 - accuracy: 0.9970\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0087 - accuracy: 0.9973\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9963\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0102 - accuracy: 0.9962\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0125 - accuracy: 0.9960\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0144 - accuracy: 0.9947\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0205 - accuracy: 0.9935\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0209 - accuracy: 0.9935\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0171 - accuracy: 0.9944\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0146 - accuracy: 0.9942\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9961\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9955\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0147 - accuracy: 0.9953\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0102 - accuracy: 0.9963\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9949\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9960\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0104 - accuracy: 0.9962\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0115 - accuracy: 0.9963\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0095 - accuracy: 0.9964\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0094 - accuracy: 0.9968\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0096 - accuracy: 0.9964\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9961\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0115 - accuracy: 0.9953\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9952\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0132 - accuracy: 0.9958\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9963\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0095 - accuracy: 0.9966\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0096 - accuracy: 0.9966\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0104 - accuracy: 0.9968\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0109 - accuracy: 0.9969\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9953\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9960\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9960\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0131 - accuracy: 0.9957\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0130 - accuracy: 0.9955\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0092 - accuracy: 0.9974\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0088 - accuracy: 0.9971\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0122 - accuracy: 0.9963\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0100 - accuracy: 0.9968\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0102 - accuracy: 0.9969\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9956\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.8084 - accuracy: 0.7974\n",
            "Epoch 1/100\n",
            "28/28 [==============================] - 3s 28ms/step - loss: 0.5150 - accuracy: 0.7012\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3897 - accuracy: 0.7900\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3471 - accuracy: 0.8185\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.3095 - accuracy: 0.8451\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2689 - accuracy: 0.8691\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.2322 - accuracy: 0.8929\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.2026 - accuracy: 0.9098\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1561 - accuracy: 0.9353\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1323 - accuracy: 0.9446\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.1091 - accuracy: 0.9582\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0863 - accuracy: 0.9657\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0747 - accuracy: 0.9721\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0642 - accuracy: 0.9763\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0527 - accuracy: 0.9810\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0459 - accuracy: 0.9831\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0415 - accuracy: 0.9855\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0365 - accuracy: 0.9878\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0334 - accuracy: 0.9878\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0299 - accuracy: 0.9889\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0261 - accuracy: 0.9909\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0300 - accuracy: 0.9888\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0274 - accuracy: 0.9899\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0234 - accuracy: 0.9916\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0265 - accuracy: 0.9904\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0273 - accuracy: 0.9905\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0202 - accuracy: 0.9923\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0184 - accuracy: 0.9929\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0197 - accuracy: 0.9928\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0221 - accuracy: 0.9919\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0198 - accuracy: 0.9933\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0166 - accuracy: 0.9943\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0211 - accuracy: 0.9928\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0221 - accuracy: 0.9926\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9930\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0216 - accuracy: 0.9926\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0199 - accuracy: 0.9931\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0186 - accuracy: 0.9935\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0171 - accuracy: 0.9940\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0204 - accuracy: 0.9934\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0212 - accuracy: 0.9928\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0190 - accuracy: 0.9938\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0167 - accuracy: 0.9940\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0167 - accuracy: 0.9950\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9956\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0117 - accuracy: 0.9963\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0116 - accuracy: 0.9963\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0139 - accuracy: 0.9946\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0145 - accuracy: 0.9950\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0141 - accuracy: 0.9952\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0093 - accuracy: 0.9969\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0108 - accuracy: 0.9965\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0095 - accuracy: 0.9965\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0115 - accuracy: 0.9959\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0101 - accuracy: 0.9967\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0093 - accuracy: 0.9968\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0113 - accuracy: 0.9966\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0126 - accuracy: 0.9957\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0116 - accuracy: 0.9961\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0126 - accuracy: 0.9956\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0131 - accuracy: 0.9956\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0137 - accuracy: 0.9949\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9951\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0178 - accuracy: 0.9940\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0154 - accuracy: 0.9942\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0174 - accuracy: 0.9943\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9949\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0152 - accuracy: 0.9944\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0159 - accuracy: 0.9946\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0161 - accuracy: 0.9943\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0155 - accuracy: 0.9945\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0140 - accuracy: 0.9946\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0132 - accuracy: 0.9953\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0165 - accuracy: 0.9948\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0168 - accuracy: 0.9939\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0150 - accuracy: 0.9947\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0144 - accuracy: 0.9952\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0102 - accuracy: 0.9965\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0128 - accuracy: 0.9957\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0119 - accuracy: 0.9958\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0102 - accuracy: 0.9964\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0118 - accuracy: 0.9962\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0083 - accuracy: 0.9972\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0066 - accuracy: 0.9978\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0121 - accuracy: 0.9960\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0100 - accuracy: 0.9968\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0095 - accuracy: 0.9963\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0095 - accuracy: 0.9966\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0101 - accuracy: 0.9968\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0085 - accuracy: 0.9971\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0134 - accuracy: 0.9959\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0111 - accuracy: 0.9962\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 0.0125 - accuracy: 0.9960\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0115 - accuracy: 0.9958\n",
            "Epoch 96/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0124 - accuracy: 0.9965\n",
            "Epoch 97/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0110 - accuracy: 0.9964\n",
            "Epoch 98/100\n",
            "28/28 [==============================] - 1s 27ms/step - loss: 0.0110 - accuracy: 0.9965\n",
            "Epoch 99/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0098 - accuracy: 0.9968\n",
            "Epoch 100/100\n",
            "28/28 [==============================] - 1s 28ms/step - loss: 0.0106 - accuracy: 0.9964\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.7960 - accuracy: 0.7924\n",
            "Epoch 1/60\n",
            "42/42 [==============================] - 3s 28ms/step - loss: 0.4806 - accuracy: 0.7279\n",
            "Epoch 2/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.3719 - accuracy: 0.8018\n",
            "Epoch 3/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.3371 - accuracy: 0.8224\n",
            "Epoch 4/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.3015 - accuracy: 0.8460\n",
            "Epoch 5/60\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.2714 - accuracy: 0.8677\n",
            "Epoch 6/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.2387 - accuracy: 0.8865\n",
            "Epoch 7/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.2006 - accuracy: 0.9091\n",
            "Epoch 8/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1696 - accuracy: 0.9236\n",
            "Epoch 9/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1461 - accuracy: 0.9374\n",
            "Epoch 10/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1184 - accuracy: 0.9504\n",
            "Epoch 11/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.1004 - accuracy: 0.9590\n",
            "Epoch 12/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0832 - accuracy: 0.9680\n",
            "Epoch 13/60\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0820 - accuracy: 0.9663\n",
            "Epoch 14/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0708 - accuracy: 0.9742\n",
            "Epoch 15/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0606 - accuracy: 0.9775\n",
            "Epoch 16/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0473 - accuracy: 0.9826\n",
            "Epoch 17/60\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0420 - accuracy: 0.9840\n",
            "Epoch 18/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0407 - accuracy: 0.9856\n",
            "Epoch 19/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0371 - accuracy: 0.9860\n",
            "Epoch 20/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0350 - accuracy: 0.9876\n",
            "Epoch 21/60\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0308 - accuracy: 0.9884\n",
            "Epoch 22/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0317 - accuracy: 0.9884\n",
            "Epoch 23/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0266 - accuracy: 0.9905\n",
            "Epoch 24/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0227 - accuracy: 0.9920\n",
            "Epoch 25/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0242 - accuracy: 0.9913\n",
            "Epoch 26/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0262 - accuracy: 0.9910\n",
            "Epoch 27/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0286 - accuracy: 0.9900\n",
            "Epoch 28/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0266 - accuracy: 0.9907\n",
            "Epoch 29/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0257 - accuracy: 0.9911\n",
            "Epoch 30/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0225 - accuracy: 0.9919\n",
            "Epoch 31/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0233 - accuracy: 0.9917\n",
            "Epoch 32/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0229 - accuracy: 0.9919\n",
            "Epoch 33/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0211 - accuracy: 0.9926\n",
            "Epoch 34/60\n",
            "42/42 [==============================] - 1s 27ms/step - loss: 0.0221 - accuracy: 0.9922\n",
            "Epoch 35/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0257 - accuracy: 0.9914\n",
            "Epoch 36/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0206 - accuracy: 0.9925\n",
            "Epoch 37/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0184 - accuracy: 0.9933\n",
            "Epoch 38/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0168 - accuracy: 0.9946\n",
            "Epoch 39/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0176 - accuracy: 0.9936\n",
            "Epoch 40/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0201 - accuracy: 0.9926\n",
            "Epoch 41/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0197 - accuracy: 0.9935\n",
            "Epoch 42/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0184 - accuracy: 0.9935\n",
            "Epoch 43/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0203 - accuracy: 0.9928\n",
            "Epoch 44/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0179 - accuracy: 0.9938\n",
            "Epoch 45/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0204 - accuracy: 0.9923\n",
            "Epoch 46/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0249 - accuracy: 0.9918\n",
            "Epoch 47/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0208 - accuracy: 0.9927\n",
            "Epoch 48/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0197 - accuracy: 0.9933\n",
            "Epoch 49/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0211 - accuracy: 0.9932\n",
            "Epoch 50/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0218 - accuracy: 0.9924\n",
            "Epoch 51/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0172 - accuracy: 0.9934\n",
            "Epoch 52/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0170 - accuracy: 0.9941\n",
            "Epoch 53/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0153 - accuracy: 0.9942\n",
            "Epoch 54/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0156 - accuracy: 0.9944\n",
            "Epoch 55/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0157 - accuracy: 0.9946\n",
            "Epoch 56/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0161 - accuracy: 0.9944\n",
            "Epoch 57/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0148 - accuracy: 0.9949\n",
            "Epoch 58/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0143 - accuracy: 0.9949\n",
            "Epoch 59/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0123 - accuracy: 0.9954\n",
            "Epoch 60/60\n",
            "42/42 [==============================] - 1s 28ms/step - loss: 0.0138 - accuracy: 0.9952\n",
            "5/5 [==============================] - 0s 7ms/step\n",
            ">acc=0.236, pres=0.821, rec=0.805, f1=0.812, est=0.798, cfg={'batch_size': 100, 'epochs': 60}\n",
            "Accuracy: 0.225 (0.005)\n",
            "Runtime: 20894.239 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 248 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 248 operations synced, thanks for waiting!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxm4u8Bsi9SX"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRsgbiGTc-0O"
      },
      "source": [
        "def LSTM_net():\n",
        "    inputlayer = tf.keras.layers.Input(shape=(200,2)) \n",
        "\n",
        "    lstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True))(inputlayer)\n",
        "    lstm2 = tf.keras.layers.LSTM(50)(lstm1)\n",
        "    output_layer = tf.keras.layers.Dense(units=7,activation='sigmoid')(lstm2)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputlayer, outputs=output_layer)\n",
        "\n",
        "    \n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
        "                  #,tfa.metrics.F1Score(num_classes=n_output)\n",
        "                  ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axFmOuGnNLdS"
      },
      "source": [
        "##-----------------------------------------------------------------------------------------------#\n",
        "#|  https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/ |\n",
        "##-----------------------------------------------------------------------------------------------#\n",
        "\n",
        "def run_LSTM(X_new, y, noise):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    #Neptune credentials\n",
        "    run = neptune.init(project='SSCP/SSCP2021',\n",
        "                    api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGUyN2Q2ZS05MjVkLTRlMzItODYwZS0yODQ3ZWU3ZTdmMmEifQ==')\n",
        "    run[\"Model\"] = \"LSTM-Bidir\"\n",
        "    run[\"Data\"] = \"Ca and V on separate axis\"\n",
        "    run[\"Noise\"] =  noise\n",
        "    #Define outer loop - 10-fold CV\n",
        "    cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "    y_transformed = get_labels_for_all_combinations(y)\n",
        "    run[\"CV_folds_outer_loop\"] = cv_outer.get_n_splits()\n",
        "    # enumerate splits\n",
        "    outer_results = list()\n",
        "    class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "    for k , (train_ix, test_ix) in enumerate(cv_outer.split(X_new,y_transformed)):\n",
        "        # split data\n",
        "        X_train, X_test = X_new[train_ix, :, :], X_new[test_ix, :,:]\n",
        "        y_train, y_test = y[train_ix], y[test_ix]\n",
        "        # configure the cross-validation procedure\n",
        "        cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "        #run[\"CV_folds_inner_loop\"] = cv_inner.get_n_splits()\n",
        "        # define the model\n",
        "        model = KerasClassifier(build_fn=LSTM_net, verbose=0)\n",
        "        #model.fit(X_train, y_train, verbose=1, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(X_test,y_test))\n",
        "        # define search space\n",
        "        parameters = {\n",
        "            'epochs': [20,60,100],\n",
        "            'batch_size': [20,60,100]\n",
        "        }\n",
        "        # define search\n",
        "        search = GridSearchCV(model, parameters, cv=cv_inner, refit=True)\n",
        "        # execute search\n",
        "        result = search.fit(X_train, y_train)\n",
        "        # get the best performing model fit on the whole training set\n",
        "        best_model = result.best_estimator_\n",
        "        # evaluate model on the hold out dataset\n",
        "        yhat = (best_model.predict_proba(X_test) > 0.5) * 1\n",
        "        # evaluate the model\n",
        "        acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "        pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "        # store the result\n",
        "        outer_results.append([acc, pres, rec, f1])\n",
        "        # report progress\n",
        "        print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "        #log params to Neptune\n",
        "        run[\"hyperparameters\"].log(result.best_params_)\n",
        "        run[\"accuracy_inner_loop\"].log(result.best_score_)\n",
        "        run[\"accuracy_outer_loop\"].log(acc)\n",
        "        run[\"precision_outer_loop\"].log(pres)\n",
        "        run[\"recall_outer_loop\"].log(rec)\n",
        "        run[\"fscore_outer_loop\"].log(f1)\n",
        "        #finds and logs the evaluation for each outnode\n",
        "        for channel in range(len(label_order)):\n",
        "            curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "            curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "            run[\"accuracy_outer_loop_\"+label_order[channel]].log(curr_acc)\n",
        "            run[\"precision_outer_loop_\"+label_order[channel]].log(curr_pres)\n",
        "            run[\"recall_outer_loop_\"+label_order[channel]].log(curr_rec)\n",
        "            run[\"fscore_outer_loop_\"+label_order[channel]].log(curr_f1)\n",
        "            class_results[k,channel,0] = curr_acc\n",
        "            class_results[k,channel,1] = curr_pres\n",
        "            class_results[k,channel,2] = curr_rec\n",
        "            class_results[k,channel,3] = curr_f1\n",
        "\n",
        "    # summarize the estimated performance of the model\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean([i[0] for i in outer_results]), np.std([i[3] for i in outer_results])))\n",
        "    run[\"accuracy_mean\"] = np.mean([i[0] for i in outer_results])\n",
        "    run[\"accuracy_SD\"] =  np.std([i[0] for i in outer_results])\n",
        "    run[\"precision_mean\"] = np.mean([i[1] for i in outer_results])\n",
        "    run[\"precision_SD\"] =  np.std([i[1] for i in outer_results])\n",
        "    run[\"recall_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"recall_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    run[\"F1_mean\"] = np.mean([i[2] for i in outer_results])\n",
        "    run[\"F1_SD\"] =  np.std([i[2] for i in outer_results])\n",
        "    for num_score, score in enumerate([\"accuracy\",\"presicion\",\"recall\", \"F1-score\"]):\n",
        "        for channel in range(len(label_order)):\n",
        "            run[score + \"_\" + label_order[channel] + \"_mean\"] =  class_results[:,channel,num_score].mean()\n",
        "            run[score + \"_\" + label_order[channel] + \"_SD\"] =  class_results[:,channel,num_score].std()\n",
        "\n",
        "    \n",
        "    runtime = time.time() - time_start \n",
        "    print('Runtime: %.3f sec' % runtime)\n",
        "    run[\"Runtime\"] = runtime\n",
        "    run.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV_ybi2cOU86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2499f686-21f1-4675-8e06-f5aa34aefde6"
      },
      "source": [
        "run_LSTM(X_new, y, noise)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/SSCP/SSCP2021/e/SSCP-228\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
            ">acc=0.159, pres=0.785, rec=0.786, f1=0.785, est=0.795, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.135, pres=0.766, rec=0.788, f1=0.777, est=0.797, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.190, pres=0.795, rec=0.816, f1=0.805, est=0.797, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.240, pres=0.835, rec=0.812, f1=0.822, est=0.783, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.186, pres=0.819, rec=0.798, f1=0.807, est=0.782, cfg={'batch_size': 100, 'epochs': 100}\n",
            ">acc=0.087, pres=0.740, rec=0.703, f1=0.716, est=0.783, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.194, pres=0.808, rec=0.775, f1=0.790, est=0.790, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.120, pres=0.749, rec=0.789, f1=0.763, est=0.797, cfg={'batch_size': 60, 'epochs': 100}\n",
            ">acc=0.216, pres=0.813, rec=0.802, f1=0.806, est=0.782, cfg={'batch_size': 20, 'epochs': 100}\n",
            ">acc=0.245, pres=0.829, rec=0.784, f1=0.805, est=0.807, cfg={'batch_size': 100, 'epochs': 100}\n",
            "Accuracy: 0.177 (0.029)\n",
            "Runtime: 41478.995 sec\n",
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Waiting for the remaining 106 operations to synchronize with Neptune. Do not kill this process.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All 106 operations synced, thanks for waiting!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8OQrrtyOrqT"
      },
      "source": [
        "## Run CNN and LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSW93zygOX_T"
      },
      "source": [
        "#noises = [2, 3, 4, 5]\n",
        "noises = [2,4,5]\n",
        "for i in range(len(noises)):\n",
        "    X_curr = add_noise_and_normalize_new_axis(V_c, V_d, Ca_c, Ca_d, pct_noise=noises[i])\n",
        "\n",
        "    #print(\"\\nCNN, noise={}%:\".format(noises[i]))\n",
        "    #run_CNN(X_curr, y, noises[i])\n",
        "\n",
        "    print(\"\\nLSTM, noise={}%:\".format(noises[i]))\n",
        "    run_LSTM(X_curr, y, noises[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHbhotxAeCWS"
      },
      "source": [
        "# Experiments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P1PZKh2BNjd",
        "outputId": "f6ff1d31-07f9-4cb1-8014-e8098052523b"
      },
      "source": [
        "!pip install -U keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz07zkabBbof",
        "outputId": "bea99e17-877f-456a-f257-83528277beb5"
      },
      "source": [
        "!pip install arff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arff in /usr/local/lib/python3.7/dist-packages (0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOQ0z9brelQY"
      },
      "source": [
        "def create_model_multiclass(input_dim, output_dim):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOuE4XtvBS7z"
      },
      "source": [
        "KERAS_PARAMS = dict(epochs=20, batch_size=30, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr0kPlC9Cu00"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjHiXO1mBDK_"
      },
      "source": [
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.ext import Keras\n",
        "\n",
        "clf = BinaryRelevance(classifier=Keras(create_model_multiclass, True, KERAS_PARAMS), require_dense=[True,True])\n",
        "clf.fit(X,y)\n",
        "y_pred = clf.predict(X_super_test)\n",
        "acc = f1_score(y_super_test, y_pred, average='macro')\n",
        "print(f\"F1-score: {acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZZihPA6FAY9"
      },
      "source": [
        "def encoder_model_soft(input_dim, output_dim):\n",
        "    X_input=tf.keras.layers.Input(shape=(input_dim))\n",
        "     # conv block -1\n",
        "    reshape = tf.keras.layers.Reshape((200,2))(X_input)\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=32,kernel_size=5,strides=1,padding='same')(reshape)\n",
        "    conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "    conv1 = tf.keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "    conv1 = tf.keras.layers.Dropout(rate=0.2)(conv1)\n",
        "    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "    # conv block -2\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=64,kernel_size=11,strides=1,padding='same')(conv1)\n",
        "    conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "    conv2 = tf.keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "    conv2 = tf.keras.layers.Dropout(rate=0.2)(conv2)\n",
        "    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "    # conv block -3\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=128,kernel_size=21,strides=1,padding='same')(conv2)\n",
        "    conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "    conv3 = tf.keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "    conv3 = tf.keras.layers.Dropout(rate=0.2)(conv3)\n",
        "    # split for attention\n",
        "    attention_data = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n",
        "    attention_softmax = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n",
        "    # attention mechanism\n",
        "    attention_softmax = tf.keras.layers.Softmax()(attention_softmax)\n",
        "    multiply_layer = tf.keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "    # last layer\n",
        "    dense_layer = tf.keras.layers.Dense(units=128,activation='relu')(multiply_layer)\n",
        "    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "    # output layer\n",
        "    flatten_layer = tf.keras.layers.Flatten()(dense_layer)\n",
        "    output_layer = tf.keras.layers.Dense(units=output_dim,activation='softmax')(flatten_layer)\n",
        "\n",
        "    model = tf.keras.Model(inputs=X_input, outputs=output_layer)\n",
        "\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqvhtsXuQbc7",
        "outputId": "ae787736-e033-488c-f680-8ed9924bf4d4"
      },
      "source": [
        "from skmultilearn.dataset import load_dataset\n",
        "\n",
        "X_train, y_train, feature_names, label_names = load_dataset('emotions', 'train')\n",
        "X_test, y_test, _, _ = load_dataset('emotions', 'test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emotions:train - exists, not redownloading\n",
            "emotions:test - exists, not redownloading\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASlqu-XdRGzY",
        "outputId": "2751a4e2-df3a-40b8-b4ba-0d7ae02a75d8"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<391x6 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 709 stored elements in List of Lists format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9R7-sH1QmxB"
      },
      "source": [
        "\n",
        "def create_model_multiclass(input_dim, output_dim):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='softmax'))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PMZ2J2kQrKW"
      },
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "clf = LabelPowerset(classifier=Keras(create_model_multiclass, True, KERAS_PARAMS), require_dense=[True,True])\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGCs1Zs8RcDc"
      },
      "source": [
        "from scipy import sparse\n",
        "sX = sparse.csr_matrix(X,dtype=np.int64)\n",
        "sy = sparse.csr_matrix(y,dtype=np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO2oGNRDSF0V",
        "outputId": "363b152a-0fc2-4a20-d094-c67d265f0bc0"
      },
      "source": [
        "sy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2908x7 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 10940 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdZg2yxSFhlg",
        "outputId": "a730f47a-21fe-4977-ab5b-5b464f5dfbfd"
      },
      "source": [
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.ext import Keras\n",
        "#y = labels\n",
        "#X, y, X_super_test, y_super_test = iterative_train_test_split(X_new, y, test_size = ratio)\n",
        "clf = LabelPowerset(classifier=Keras(encoder_model_soft, True, KERAS_PARAMS), require_dense=[True,True])\n",
        "clf.fit(sX,sy)\n",
        "#y_pred = clf.predict(X_super_test)\n",
        "#acc = f1_score(y_super_test, y_pred, average='macro')\n",
        "#print(f\"F1-score: {acc}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "97/97 [==============================] - 12s 108ms/step - loss: 4.6640 - accuracy: 0.0506\n",
            "Epoch 2/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 3.6461 - accuracy: 0.0970\n",
            "Epoch 3/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 3.2334 - accuracy: 0.1475\n",
            "Epoch 4/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 2.8684 - accuracy: 0.2084\n",
            "Epoch 5/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 2.5994 - accuracy: 0.2679\n",
            "Epoch 6/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 2.3246 - accuracy: 0.3215\n",
            "Epoch 7/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 2.1092 - accuracy: 0.3879\n",
            "Epoch 8/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.8732 - accuracy: 0.4752\n",
            "Epoch 9/20\n",
            "97/97 [==============================] - 10s 106ms/step - loss: 1.7030 - accuracy: 0.5199\n",
            "Epoch 10/20\n",
            "97/97 [==============================] - 10s 106ms/step - loss: 1.5644 - accuracy: 0.5512\n",
            "Epoch 11/20\n",
            "97/97 [==============================] - 10s 108ms/step - loss: 1.4441 - accuracy: 0.5990\n",
            "Epoch 12/20\n",
            "97/97 [==============================] - 11s 109ms/step - loss: 1.3611 - accuracy: 0.6176\n",
            "Epoch 13/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.2837 - accuracy: 0.6541\n",
            "Epoch 14/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.2469 - accuracy: 0.6468\n",
            "Epoch 15/20\n",
            "97/97 [==============================] - 10s 108ms/step - loss: 1.1980 - accuracy: 0.6633\n",
            "Epoch 16/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.1835 - accuracy: 0.6743\n",
            "Epoch 17/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.1052 - accuracy: 0.6960\n",
            "Epoch 18/20\n",
            "97/97 [==============================] - 10s 106ms/step - loss: 1.1077 - accuracy: 0.6926\n",
            "Epoch 19/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.0921 - accuracy: 0.6974\n",
            "Epoch 20/20\n",
            "97/97 [==============================] - 10s 107ms/step - loss: 1.0475 - accuracy: 0.7118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelPowerset(classifier=Keras(build_function=<function encoder_model_soft at 0x7f92b50e0c20>,\n",
              "                               keras_params={'batch_size': 30, 'epochs': 20,\n",
              "                                             'verbose': 1},\n",
              "                               multi_class=True),\n",
              "              require_dense=[True, True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Bmixe024POhE",
        "outputId": "02017ef1-9fbe-48b4-b132-e137028eae89"
      },
      "source": [
        "y_pred = clf.predict(X_super_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4f4b94347e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_super_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skmultilearn/problem_transform/br.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    180\u001b[0m         predictions = [self._ensure_multi_label_from_single_class(\n\u001b[1;32m    181\u001b[0m             self.classifiers_[label].predict(self._ensure_input_format(X)))\n\u001b[0;32m--> 182\u001b[0;31m             for label in range(self.model_count_)]\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skmultilearn/problem_transform/br.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m         predictions = [self._ensure_multi_label_from_single_class(\n\u001b[1;32m    181\u001b[0m             self.classifiers_[label].predict(self._ensure_input_format(X)))\n\u001b[0;32m--> 182\u001b[0;31m             for label in range(self.model_count_)]\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skmultilearn/ext/keras.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m    237\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sk_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'predict_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgKLlF5yPLD8"
      },
      "source": [
        "\n",
        "acc = f1_score(y_super_test, y_pred, average='macro')\n",
        "print(f\"F1-score: {acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ_t2yxRH6EF",
        "outputId": "30218d00-4a77-4755-813b-643ee783bec9"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2921, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5t_7jqHHA2c",
        "outputId": "1fe4c06b-5351-449c-d41b-2b9a0ebc3c15"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2921, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIxLRlsRH8VQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZmTuUlW1I6"
      },
      "source": [
        "# Explain with Lime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUOQ6kCLW20u",
        "outputId": "3c639d39-8665-4832-b9e0-e1e74ddc2416"
      },
      "source": [
        "  time_start = time.time()\n",
        "\n",
        "  #Neptune credentials\n",
        "\n",
        "  \n",
        "  #Define outer loop - 10-fold CV\n",
        "  cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "  y_transformed = get_labels_for_all_combinations(y)\n",
        " \n",
        "  # enumerate splits\n",
        "  outer_results = list()\n",
        "  class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "  for k , (train_ix, test_ix) in enumerate(cv_outer.split(X,y_transformed)):\n",
        "      # split data\n",
        "      X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
        "      y_train, y_test = y[train_ix], y[test_ix]\n",
        "      # configure the cross-validation procedure\n",
        "      cv_inner = KFold(n_splits=2, shuffle=True, random_state=1)\n",
        "      # define the model\n",
        "      model = LabelPowerset(classifier = SVC())\n",
        "      # define search space\n",
        "      parameters = {\n",
        "          'classifier': [SVC(probability=True)],\n",
        "          #'classifier__C': np.logspace(2,-2, num=3),\n",
        "          #'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "          #'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
        "          #https://stackoverflow.com/questions/36306555/scikit-learn-grid-search-with-svm-regression                 \n",
        "      }\n",
        "      # define search\n",
        "      search = GridSearchCV(model, parameters, scoring='accuracy', cv=cv_inner, refit=True)\n",
        "      #print(X_train.shape, kernel_train.shape, y_train.shape)\n",
        "\n",
        "      # execute search\n",
        "      result = search.fit(X_train, y_train)\n",
        "      # get the best performing model fit on the whole training set\n",
        "      best_model = result.best_estimator_\n",
        "\n",
        "      # evaluate model on the hold out dataset\n",
        "      yhat = best_model.predict(X_test).toarray()\n",
        "\n",
        "      # evaluate the model\n",
        "      acc = accuracy_score(y_test, (yhat>0.5)*1) #f1_score(y_test, yhat, average='macro')\n",
        "      pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "      # store the result\n",
        "      outer_results.append([acc, pres, rec, f1])\n",
        "      # report progress\n",
        "      print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "      #log params to Neptune\n",
        "\n",
        "      #finds and logs the evaluation for each outnode\n",
        "      for channel in range(len(label_order)):\n",
        "          curr_acc = accuracy_score(y_test[:,channel], yhat[:,channel]) #f1_score(y_test, yhat, average='macro')\n",
        "          curr_pres, curr_rec, curr_f1, curr_sup = precision_recall_fscore_support(y_test[channel], yhat[channel], average='macro')\n",
        "\n",
        "          class_results[k,channel,0] = curr_acc\n",
        "          class_results[k,channel,1] = curr_pres\n",
        "          class_results[k,channel,2] = curr_rec\n",
        "          class_results[k,channel,3] = curr_f1\n",
        "      #break model after one fold\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">acc=0.126, pres=0.755, rec=0.709, f1=0.731, est=0.060, cfg={'classifier': SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW8bTRvTW8yj",
        "outputId": "ceccd694-3ce0-4e79-c822-4e621f996d26"
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.7/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.62.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpzql0T1XQeC"
      },
      "source": [
        "import lime\n",
        "from lime import lime_tabular\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5_XF45cX4sl",
        "outputId": "41706843-ecab-4f8d-c894-48b70244a9e8"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(459, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YONw3wJGY-Xi",
        "outputId": "62f611d2-b2ad-4ea8-eb1a-99e1cd1229d8"
      },
      "source": [
        "X_test[:100].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4d4qhe-aDE7",
        "outputId": "7063da57-d88d-47e4-c44f-b46ec199bce3"
      },
      "source": [
        "X_test[100:101].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 400)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-lZYZg0YPpo",
        "outputId": "f853a8a1-b07a-401c-c0ef-368597c9a1ee"
      },
      "source": [
        "np.expand_dims(X_test[101],axis=1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC72ibyKgxDC",
        "outputId": "8b14ad51-5ca0-4e1e-ccda-0dbf9bcee7a4"
      },
      "source": [
        "tf.keras.utils.to_categorical(y_test[:100,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLzP-TA0XZtw"
      },
      "source": [
        "explainer = lime_tabular.RecurrentTabularExplainer(np.expand_dims(X_test[:100],axis=2),training_labels=tf.keras.utils.to_categorical(y_test[:100,0]), feature_names=[\"Ca2+ and V Signal\"],\n",
        "                                                   discretize_continuous=False, feature_selection='auto', class_names=['no block','block'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BT7QO7IjVI7"
      },
      "source": [
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_test[:100],training_labels=tf.keras.utils.to_categorical(y_test[:100,0]), feature_names=[\"Ca2+ and V Signal\"], class_names=['no block','block'], discretize_continuous=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "-gU7u184jqca",
        "outputId": "c1121e15-04fc-4b8b-c895-de33a90ecf39"
      },
      "source": [
        "exp = explainer.explain_instance(X_test[100:101], wrapped_mod.predict_proba, num_features=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-b3feb44a89fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfeature_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_and_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mfeature_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mconvert_and_round\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_and_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_and_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svLCFJ2UhD1-",
        "outputId": "603f7832-7f07-44c7-f418-ff182a14ffde"
      },
      "source": [
        "best_model.predict_proba(X_test[100:101]).toarray().ravel()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25773902348280603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GaWtjQHYpp6"
      },
      "source": [
        "class model_wrapper:\n",
        "    # drf is the h2o distributed random forest object, the column_names is the\n",
        "    # labels of the X values\n",
        "    def __init__(self,model):\n",
        "            \n",
        "            self.model = model\n",
        " \n",
        "    def predict_proba(self,input_data):        \n",
        "        # If we have just 1 row of data we need to reshape it\n",
        "\n",
        "        #input_data = input_data[:,:,0]\n",
        "\n",
        "\n",
        "        # Predict with the h2o drf\n",
        "        self.pred = self.model.predict_proba(input_data).toarray().ravel()[0]\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        return np.array([[1-self.pred,self.pred]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfc_3WkOZxwn"
      },
      "source": [
        "wrapped_mod = model_wrapper(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Pwwfb8z5Z45n",
        "outputId": "7e706f52-9aeb-4f08-b868-1b2f36dcb774"
      },
      "source": [
        "exp = explainer.explain_instance(np.expand_dims(X_test[100:101],axis=2),wrapped_mod.predict_proba, num_features=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-610c92df095e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwrapped_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mdistance_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             model_regressor=model_regressor)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    457\u001b[0m                     \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0mmodel_regressor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_regressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                     feature_selection=self.feature_selection)\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_base.py\u001b[0m in \u001b[0;36mexplain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[1;32m    185\u001b[0m                                                \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                                                \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                                                feature_selection)\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_regressor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             model_regressor = Ridge(alpha=1, fit_intercept=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_base.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mn_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'highest_weights'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             return self.feature_selection(data, labels, weights,\n\u001b[0;32m--> 135\u001b[0;31m                                           num_features, n_method)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     def explain_instance_with_data(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lime/lime_base.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[1;32m     78\u001b[0m             clf = Ridge(alpha=0.01, fit_intercept=True,\n\u001b[1;32m     79\u001b[0m                         random_state=self.random_state)\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \"\"\"\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    545\u001b[0m                          \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_accept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                          multi_output=True, y_numeric=True)\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sparse_cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5000, 1]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTO8D9P9isf3"
      },
      "source": [
        "exp.as_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irpm6Un0aMUa",
        "outputId": "00ffbf7f-a57b-4014-9275-11534312446e"
      },
      "source": [
        "best_model.predict(X_test[100:101]).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoUg_X3cbjHl",
        "outputId": "57de148e-8e87-4735-9559-f23bc532a857"
      },
      "source": [
        "1-best_model.predict_proba(X_test[100:101]).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.74226098, 0.77087529, 0.6116004 , 0.71608946, 0.63286082,\n",
              "        0.29217223, 0.40948496]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVbRCrWZyWW",
        "outputId": "b3f77f11-352b-4417-ca2c-728391e2c1a1"
      },
      "source": [
        "wrapped_mod.predict_proba(np.expand_dims(X_test[100:101],axis=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.74226098, 0.25773902])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7E6aEj1c0Hq"
      },
      "source": [
        "time_start = time.time()\n",
        "\n",
        "#Define outer loop - 10-fold CV\n",
        "cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "y_transformed = get_labels_for_all_combinations(y)\n",
        "\n",
        "# enumerate splits\n",
        "outer_results = list()\n",
        "class_results = np.zeros((cv_outer.get_n_splits(),len(label_order),4))\n",
        "for k , (train_ix, test_ix) in enumerate(cv_outer.split(X_new,y_transformed)):\n",
        "    # split data\n",
        "    X_train, X_test = X_new[train_ix, :, :], X_new[test_ix, :,:]\n",
        "    y_train, y_test = y[train_ix], y[test_ix]\n",
        "    # configure the cross-validation procedure\n",
        "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\n",
        "    # define the model\n",
        "    model = KerasClassifier(build_fn=encoder_model_2, verbose=1)\n",
        "    # define search space\n",
        "    parameters = {\n",
        "        #'epochs': [20,60,100],\n",
        "        'epochs': [20],\n",
        "        #'batch_size': [20,60,100]\n",
        "        'batch_size': [20] \n",
        "    }\n",
        "    # define search\n",
        "    search = GridSearchCV(model, parameters, cv=cv_inner, refit=True)\n",
        "    #search = GridSearchCV(estimator=model, param_grid=parameters)\n",
        "    # execute search\n",
        "    result = search.fit(X_train, y_train)\n",
        "    # get the best performing model fit on the whole training set\n",
        "    best_model = result.best_estimator_\n",
        "    # evaluate model on the hold out dataset\n",
        "\n",
        "    yhat = (best_model.predict_proba(X_test) > 0.5) * 1\n",
        "    # evaluate the model\n",
        "    acc = accuracy_score(y_test, yhat) #f1_score(y_test, yhat, average='macro')\n",
        "    pres, rec, f1, sup = precision_recall_fscore_support(y_test, yhat, average='macro')\n",
        "    # store the result\n",
        "    outer_results.append([acc, pres, rec, f1])\n",
        "    # report progress\n",
        "    print('>acc=%.3f, pres=%.3f, rec=%.3f, f1=%.3f, est=%.3f, cfg=%s' % (acc, pres, rec, f1, result.best_score_, result.best_params_))\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}